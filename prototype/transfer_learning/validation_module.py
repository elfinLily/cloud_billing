# -*- coding: utf-8 -*-
"""
Transfer Learning ê²€ì¦ ëª¨ë“ˆ

GCP ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¶”ì • ì •í™•ë„ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
ì‹¤ì œ ì‚¬ìš©ë¥  vs ì¶”ì • ì‚¬ìš©ë¥  ë¹„êµ
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import sys

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'data_processing'))

from pipeline_base import PipelineBase


class TransferLearningValidator(PipelineBase):
    """
    Transfer Learning ê²€ì¦ í´ë˜ìŠ¤
    
    ê²€ì¦ ë°©ë²•:
    1. GCP ë°ì´í„°ë¥¼ Train/Testë¡œ ë¶„í• 
    2. Train ë°ì´í„°ë¡œ íŒ¨í„´ í•™ìŠµ
    3. Test ë°ì´í„°ë¡œ ì¶”ì •
    4. ì‹¤ì œ vs ì¶”ì • ë¹„êµ
    
    í‰ê°€ ì§€í‘œ:
    - MAE (Mean Absolute Error)
    - RMSE (Root Mean Squared Error)
    - RÂ² Score
    - MAPE (Mean Absolute Percentage Error)
    """
    
    def __init__(self, config_path='config/focus_config.yaml'):
        """
        ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        super().__init__(config_path)
        
        # ê²½ë¡œ ì„¤ì •
        data_config = self.config['data']
        self.gcp_data_path = Path(data_config['gcp_raw_path'])
        self.output_path = Path('results/transfer_learning/validation_results.json')
        
        # ë°ì´í„°
        self.df_gcp = None
        self.df_train = None
        self.df_test = None
        self.train_patterns = None
        self.validation_results = None
        
        # ê²€ì¦ ì„¤ì •
        self.test_ratio = 0.2  # 20% í…ŒìŠ¤íŠ¸
        self.random_state = 42
    
    
    def load(self):
        """
        GCP ë°ì´í„° ë¡œë“œ
        
        Returns:
            self
        """
        self.print_step("GCP ë°ì´í„° ë¡œë”©", f"{self.gcp_data_path}")
        
        if not self.gcp_data_path.exists():
            self.print_error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.gcp_data_path}")
            raise FileNotFoundError(f"{self.gcp_data_path}")
        
        # CSV ë¡œë“œ
        self.df_gcp = pd.read_csv(self.gcp_data_path)
        
        self.print_success("ë¡œë“œ ì™„ë£Œ")
        print(f"   ğŸ“Š ë ˆì½”ë“œ: {len(self.df_gcp):,}ê±´")
        
        return self
    
    
    def _find_usage_columns(self):
        """
        CPU/Memory ì‚¬ìš©ë¥  ì»¬ëŸ¼ ì°¾ê¸°
        
        Returns:
            tuple: (cpu_col, memory_col)
        """
        # CPU ì»¬ëŸ¼ ì°¾ê¸°
        cpu_cols = [col for col in self.df_gcp.columns 
                   if 'cpu' in col.lower() and ('usage' in col.lower() or 'utilization' in col.lower())]
        
        # Memory ì»¬ëŸ¼ ì°¾ê¸°
        memory_cols = [col for col in self.df_gcp.columns 
                      if 'memory' in col.lower() and ('usage' in col.lower() or 'utilization' in col.lower())]
        
        cpu_col = cpu_cols[0] if cpu_cols else None
        memory_col = memory_cols[0] if memory_cols else None
        
        print(f"\n   ğŸ” ë°œê²¬ëœ ì»¬ëŸ¼:")
        print(f"      â€¢ CPU: {cpu_col}")
        print(f"      â€¢ Memory: {memory_col}")
        
        return cpu_col, memory_col
    
    
    def _find_service_column(self):
        """
        ì„œë¹„ìŠ¤ëª… ì»¬ëŸ¼ ì°¾ê¸°
        
        Returns:
            str: ì„œë¹„ìŠ¤ëª… ì»¬ëŸ¼
        """
        service_cols = [col for col in self.df_gcp.columns 
                       if 'service' in col.lower() and 'name' in col.lower()]
        
        service_col = service_cols[0] if service_cols else None
        print(f"      â€¢ Service: {service_col}")
        
        return service_col
    
    
    def split_data(self):
        """
        ë°ì´í„°ë¥¼ Train/Testë¡œ ë¶„í• 
        
        ë¶„í•  ê¸°ì¤€: ì„œë¹„ìŠ¤ë³„ Stratified Split
        
        Returns:
            self
        """
        self.print_step("Train/Test ë¶„í• ")
        
        # ì»¬ëŸ¼ ì°¾ê¸°
        self.cpu_col, self.memory_col = self._find_usage_columns()
        self.service_col = self._find_service_column()
        
        if not self.cpu_col or not self.memory_col or not self.service_col:
            self.print_error("í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return self
        
        # ë°ì´í„° ì •ì œ
        df_clean = self.df_gcp[
            self.df_gcp[self.cpu_col].notna() &
            self.df_gcp[self.memory_col].notna() &
            self.df_gcp[self.service_col].notna()
        ].copy()
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        if df_clean[self.cpu_col].max() > 1.5:
            df_clean[self.cpu_col] = df_clean[self.cpu_col] / 100.0
        if df_clean[self.memory_col].max() > 1.5:
            df_clean[self.memory_col] = df_clean[self.memory_col] / 100.0
        
        # 0 ì œê±°
        df_clean = df_clean[
            (df_clean[self.cpu_col] > 0) & 
            (df_clean[self.memory_col] > 0)
        ]
        
        print(f"\n   ğŸ“Š ì •ì œëœ ë°ì´í„°: {len(df_clean):,}ê±´")
        
        # ì„œë¹„ìŠ¤ë³„ ë¶„í• 
        services = df_clean[self.service_col].unique()
        
        train_indices = []
        test_indices = []
        
        np.random.seed(self.random_state)
        
        for service in services:
            service_indices = df_clean[df_clean[self.service_col] == service].index.tolist()
            
            if len(service_indices) < 5:
                # ìƒ˜í”Œì´ ì ìœ¼ë©´ ëª¨ë‘ Trainìœ¼ë¡œ
                train_indices.extend(service_indices)
            else:
                # Shuffle
                np.random.shuffle(service_indices)
                
                # Split
                split_idx = int(len(service_indices) * (1 - self.test_ratio))
                train_indices.extend(service_indices[:split_idx])
                test_indices.extend(service_indices[split_idx:])
        
        self.df_train = df_clean.loc[train_indices].copy()
        self.df_test = df_clean.loc[test_indices].copy()
        
        self.print_success("ë¶„í•  ì™„ë£Œ")
        print(f"   â€¢ Train: {len(self.df_train):,}ê±´ ({len(self.df_train)/len(df_clean)*100:.1f}%)")
        print(f"   â€¢ Test: {len(self.df_test):,}ê±´ ({len(self.df_test)/len(df_clean)*100:.1f}%)")
        print(f"   â€¢ Train ì„œë¹„ìŠ¤: {self.df_train[self.service_col].nunique()}ê°œ")
        print(f"   â€¢ Test ì„œë¹„ìŠ¤: {self.df_test[self.service_col].nunique()}ê°œ")
        
        return self
    
    
    def learn_train_patterns(self):
        """
        Train ë°ì´í„°ì—ì„œ íŒ¨í„´ í•™ìŠµ
        
        Returns:
            self
        """
        self.print_step("Train ë°ì´í„° íŒ¨í„´ í•™ìŠµ")
        
        if self.df_train is None:
            self.print_error("ë¨¼ì € split_data()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return self
        
        # ì„œë¹„ìŠ¤ë³„ íŒ¨í„´ í•™ìŠµ
        self.train_patterns = {}
        
        grouped = self.df_train.groupby(self.service_col)
        
        for service, group in grouped:
            pattern = {
                'service_name': service,
                'sample_count': len(group)
            }
            
            # CPU í†µê³„
            cpu_data = group[self.cpu_col].dropna()
            if len(cpu_data) > 0:
                pattern['cpu'] = {
                    'mean': float(cpu_data.mean()),
                    'std': float(cpu_data.std()),
                    'median': float(cpu_data.median())
                }
            
            # Memory í†µê³„
            mem_data = group[self.memory_col].dropna()
            if len(mem_data) > 0:
                pattern['memory'] = {
                    'mean': float(mem_data.mean()),
                    'std': float(mem_data.std()),
                    'median': float(mem_data.median())
                }
            
            self.train_patterns[service] = pattern
        
        self.print_success(f"íŒ¨í„´ í•™ìŠµ ì™„ë£Œ: {len(self.train_patterns)}ê°œ ì„œë¹„ìŠ¤")
        
        return self
    
    
    def _get_global_average(self):
        """
        ì „ì²´ í‰ê·  íŒ¨í„´ ê³„ì‚° (Fallbackìš©)
        
        Returns:
            dict: í‰ê·  íŒ¨í„´
        """
        cpu_means = [p['cpu']['mean'] for p in self.train_patterns.values() if 'cpu' in p]
        mem_means = [p['memory']['mean'] for p in self.train_patterns.values() if 'memory' in p]
        
        return {
            'cpu': {'mean': np.mean(cpu_means)},
            'memory': {'mean': np.mean(mem_means)}
        }
    
    
    def process(self):
        """
        Test ë°ì´í„°ë¡œ ê²€ì¦ ìˆ˜í–‰
        
        Returns:
            self
        """
        self.print_step("ê²€ì¦ ìˆ˜í–‰")
        
        if self.df_test is None or self.train_patterns is None:
            self.print_error("ë¨¼ì € split_data()ì™€ learn_train_patterns()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return self
        
        # ì¶”ì •ê°’ ê³„ì‚°
        estimated_cpu = []
        estimated_memory = []
        actual_cpu = []
        actual_memory = []
        match_methods = []
        
        global_avg = self._get_global_average()
        
        for idx, row in self.df_test.iterrows():
            service = row[self.service_col]
            
            # ì‹¤ì œê°’
            actual_cpu.append(row[self.cpu_col])
            actual_memory.append(row[self.memory_col])
            
            # ì¶”ì •ê°’
            if service in self.train_patterns:
                pattern = self.train_patterns[service]
                estimated_cpu.append(pattern['cpu']['mean'])
                estimated_memory.append(pattern['memory']['mean'])
                match_methods.append('exact_match')
            else:
                # Fallback: ì „ì²´ í‰ê· 
                estimated_cpu.append(global_avg['cpu']['mean'])
                estimated_memory.append(global_avg['memory']['mean'])
                match_methods.append('global_average')
        
        # ê²°ê³¼ DataFrame
        df_results = pd.DataFrame({
            'actual_cpu': actual_cpu,
            'estimated_cpu': estimated_cpu,
            'actual_memory': actual_memory,
            'estimated_memory': estimated_memory,
            'match_method': match_methods
        })
        
        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        self.validation_results = self._calculate_metrics(df_results)
        self.validation_results['df_comparison'] = df_results
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_validation_results()
        
        self.result = self.validation_results
        
        return self
    
    
    def _calculate_metrics(self, df_results):
        """
        í‰ê°€ ì§€í‘œ ê³„ì‚°
        
        Args:
            df_results: ì‹¤ì œ/ì¶”ì • ë¹„êµ DataFrame
        
        Returns:
            dict: í‰ê°€ ì§€í‘œ
        """
        metrics = {}
        
        # ì „ì²´ ë©”íŠ¸ë¦­
        for target in ['cpu', 'memory']:
            actual = df_results[f'actual_{target}'].values
            estimated = df_results[f'estimated_{target}'].values
            
            # MAE
            mae = mean_absolute_error(actual, estimated)
            
            # RMSE
            rmse = np.sqrt(mean_squared_error(actual, estimated))
            
            # RÂ² Score
            r2 = r2_score(actual, estimated)
            
            # MAPE
            mape = np.mean(np.abs((actual - estimated) / actual)) * 100
            
            metrics[target] = {
                'mae': mae,
                'rmse': rmse,
                'r2': r2,
                'mape': mape
            }
        
        # ë§¤ì¹­ ë°©ë²•ë³„ ë©”íŠ¸ë¦­
        for method in ['exact_match', 'global_average']:
            df_method = df_results[df_results['match_method'] == method]
            
            if len(df_method) == 0:
                continue
            
            for target in ['cpu', 'memory']:
                actual = df_method[f'actual_{target}'].values
                estimated = df_method[f'estimated_{target}'].values
                
                mae = mean_absolute_error(actual, estimated)
                
                metrics[f'{target}_{method}'] = {
                    'mae': mae,
                    'count': len(df_method)
                }
        
        # ë©”íƒ€ ì •ë³´
        metrics['total_samples'] = len(df_results)
        metrics['exact_match_count'] = (df_results['match_method'] == 'exact_match').sum()
        metrics['global_average_count'] = (df_results['match_method'] == 'global_average').sum()
        metrics['exact_match_ratio'] = metrics['exact_match_count'] / metrics['total_samples'] * 100
        
        return metrics
    
    def process_service_level(self):
        """
        ì„œë¹„ìŠ¤ ìˆ˜ì¤€ì—ì„œ ê²€ì¦ ìˆ˜í–‰ (ê°œì„ ëœ ë°©ì‹)
        
        ê°œë³„ ë ˆì½”ë“œê°€ ì•„ë‹Œ ì„œë¹„ìŠ¤ë³„ í‰ê· ì„ ë¹„êµ
        
        Returns:
            self
        """
        self.print_step("ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ê²€ì¦ ìˆ˜í–‰ (ê°œì„ ëœ ë°©ì‹)")
        
        if self.df_test is None or self.train_patterns is None:
            self.print_error("ë¨¼ì € split_data()ì™€ learn_train_patterns()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return self
        
        # Test ë°ì´í„°ì˜ ì„œë¹„ìŠ¤ë³„ ì‹¤ì œ í‰ê·  ê³„ì‚°
        test_service_avg = self.df_test.groupby(self.service_col).agg({
            self.cpu_col: 'mean',
            self.memory_col: 'mean'
        }).reset_index()
        
        test_service_avg.columns = ['service', 'actual_cpu_avg', 'actual_memory_avg']
        
        # Train íŒ¨í„´ì—ì„œ ì¶”ì •ê°’ ê°€ì ¸ì˜¤ê¸°
        estimated_cpu = []
        estimated_memory = []
        match_methods = []
        
        global_avg = self._get_global_average()
        
        for service in test_service_avg['service']:
            if service in self.train_patterns:
                pattern = self.train_patterns[service]
                estimated_cpu.append(pattern['cpu']['mean'])
                estimated_memory.append(pattern['memory']['mean'])
                match_methods.append('exact_match')
            else:
                estimated_cpu.append(global_avg['cpu']['mean'])
                estimated_memory.append(global_avg['memory']['mean'])
                match_methods.append('global_average')
        
        test_service_avg['estimated_cpu_avg'] = estimated_cpu
        test_service_avg['estimated_memory_avg'] = estimated_memory
        test_service_avg['match_method'] = match_methods
        
        # ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ë©”íŠ¸ë¦­ ê³„ì‚°
        service_metrics = self._calculate_service_level_metrics(test_service_avg)
        
        # ê²°ê³¼ ì €ì¥
        if self.validation_results is None:
            self.validation_results = {}
        
        self.validation_results['service_level'] = service_metrics
        self.validation_results['df_service_comparison'] = test_service_avg
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_service_level_results(service_metrics, test_service_avg)
        
        return self
    
    
    def _calculate_service_level_metrics(self, df_service):
        """
        ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Args:
            df_service: ì„œë¹„ìŠ¤ë³„ ë¹„êµ DataFrame
        
        Returns:
            dict: ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ë©”íŠ¸ë¦­
        """
        metrics = {}
        
        for target in ['cpu', 'memory']:
            actual = df_service[f'actual_{target}_avg'].values
            estimated = df_service[f'estimated_{target}_avg'].values
            
            # MAE
            mae = mean_absolute_error(actual, estimated)
            
            # RMSE
            rmse = np.sqrt(mean_squared_error(actual, estimated))
            
            # RÂ² Score
            r2 = r2_score(actual, estimated)
            
            # MAPE (0 ì œì™¸)
            mask = actual > 0.01
            if mask.sum() > 0:
                mape = np.mean(np.abs((actual[mask] - estimated[mask]) / actual[mask])) * 100
            else:
                mape = 0
            
            metrics[target] = {
                'mae': mae,
                'rmse': rmse,
                'r2': r2,
                'mape': mape
            }
        
        metrics['total_services'] = len(df_service)
        metrics['exact_match_count'] = (df_service['match_method'] == 'exact_match').sum()
        metrics['exact_match_ratio'] = metrics['exact_match_count'] / metrics['total_services'] * 100
        
        return metrics
    
    
    def _print_service_level_results(self, metrics, df_service):
        """ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*100}")
        print("ğŸ“Š ì„œë¹„ìŠ¤ ìˆ˜ì¤€ Transfer Learning ê²€ì¦ ê²°ê³¼ (ê°œì„ ëœ ë°©ì‹)")
        print(f"{'='*100}")
        
        print(f"\n   ğŸ“Œ ì„œë¹„ìŠ¤ ì •ë³´:")
        print(f"      â€¢ ì´ ì„œë¹„ìŠ¤: {metrics['total_services']}ê°œ")
        print(f"      â€¢ Exact Match: {metrics['exact_match_count']}ê°œ ({metrics['exact_match_ratio']:.1f}%)")
        
        # CPU ë©”íŠ¸ë¦­
        print(f"\n   ğŸ“Œ CPU ì‚¬ìš©ë¥  ì¶”ì • ì •í™•ë„ (ì„œë¹„ìŠ¤ í‰ê· ):")
        cpu = metrics['cpu']
        print(f"      â€¢ MAE: {cpu['mae']*100:.2f}% (ì ˆëŒ€ ì˜¤ì°¨)")
        print(f"      â€¢ RMSE: {cpu['rmse']*100:.2f}%")
        print(f"      â€¢ RÂ² Score: {cpu['r2']:.4f}")
        print(f"      â€¢ MAPE: {cpu['mape']:.2f}%")
        
        # Memory ë©”íŠ¸ë¦­
        print(f"\n   ğŸ“Œ Memory ì‚¬ìš©ë¥  ì¶”ì • ì •í™•ë„ (ì„œë¹„ìŠ¤ í‰ê· ):")
        mem = metrics['memory']
        print(f"      â€¢ MAE: {mem['mae']*100:.2f}% (ì ˆëŒ€ ì˜¤ì°¨)")
        print(f"      â€¢ RMSE: {mem['rmse']*100:.2f}%")
        print(f"      â€¢ RÂ² Score: {mem['r2']:.4f}")
        print(f"      â€¢ MAPE: {mem['mape']:.2f}%")
        
        # í•´ì„
        print(f"\n   ğŸ“Œ í•´ì„:")
        avg_mae = (cpu['mae'] + mem['mae']) / 2 * 100
        avg_r2 = (cpu['r2'] + mem['r2']) / 2
        
        if avg_r2 > 0.7:
            print(f"      ğŸ‰ ë§¤ìš° ìš°ìˆ˜! RÂ² > 0.7")
        elif avg_r2 > 0.5:
            print(f"      âœ… ì–‘í˜¸! RÂ² > 0.5")
        elif avg_r2 > 0.3:
            print(f"      âš ï¸ ë³´í†µ ìˆ˜ì¤€ (RÂ² > 0.3)")
        else:
            print(f"      âŒ ì„œë¹„ìŠ¤ íŠ¹ì„±ì´ ë‹¤ë¦„ (RÂ² < 0.3)")
        
        if avg_mae < 5:
            print(f"      ğŸ‰ í‰ê·  ì˜¤ì°¨ < 5% - ì‹¤ë¬´ ì ìš© ê°€ëŠ¥!")
        elif avg_mae < 10:
            print(f"      âœ… í‰ê·  ì˜¤ì°¨ 5-10% - ì°¸ê³ ìš©ìœ¼ë¡œ í™œìš©")
        else:
            print(f"      âš ï¸ í‰ê·  ì˜¤ì°¨ > 10% - ì£¼ì˜ í•„ìš”")
        
        # ìƒìœ„ 5ê°œ ì •í™•í•œ ì„œë¹„ìŠ¤
        df_service['cpu_error'] = abs(df_service['actual_cpu_avg'] - df_service['estimated_cpu_avg'])
        top_accurate = df_service.nsmallest(5, 'cpu_error')
        
        print(f"\n   ğŸ“ˆ ê°€ì¥ ì •í™•í•œ ì„œë¹„ìŠ¤ Top 5:")
        for i, row in top_accurate.iterrows():
            service = row['service'][:30]
            error = row['cpu_error'] * 100
            print(f"      â€¢ {service:30s}: ì˜¤ì°¨ {error:.2f}%")
        
        print(f"\n{'='*100}")

    def _print_validation_results(self):
        """ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*100}")
        print("ğŸ“Š Transfer Learning ê²€ì¦ ê²°ê³¼")
        print(f"{'='*100}")
        
        metrics = self.validation_results
        
        # ìƒ˜í”Œ ì •ë³´
        print(f"\n   ğŸ“Œ ìƒ˜í”Œ ì •ë³´:")
        print(f"      â€¢ ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {metrics['total_samples']:,}ê±´")
        print(f"      â€¢ Exact Match: {metrics['exact_match_count']:,}ê±´ ({metrics['exact_match_ratio']:.1f}%)")
        print(f"      â€¢ Global Average: {metrics['global_average_count']:,}ê±´")
        
        # CPU ë©”íŠ¸ë¦­
        print(f"\n   ğŸ“Œ CPU ì‚¬ìš©ë¥  ì¶”ì • ì •í™•ë„:")
        cpu = metrics['cpu']
        print(f"      â€¢ MAE: {cpu['mae']*100:.2f}% (ì ˆëŒ€ ì˜¤ì°¨)")
        print(f"      â€¢ RMSE: {cpu['rmse']*100:.2f}%")
        print(f"      â€¢ RÂ² Score: {cpu['r2']:.4f}")
        print(f"      â€¢ MAPE: {cpu['mape']:.2f}%")
        
        # Memory ë©”íŠ¸ë¦­
        print(f"\n   ğŸ“Œ Memory ì‚¬ìš©ë¥  ì¶”ì • ì •í™•ë„:")
        mem = metrics['memory']
        print(f"      â€¢ MAE: {mem['mae']*100:.2f}% (ì ˆëŒ€ ì˜¤ì°¨)")
        print(f"      â€¢ RMSE: {mem['rmse']*100:.2f}%")
        print(f"      â€¢ RÂ² Score: {mem['r2']:.4f}")
        print(f"      â€¢ MAPE: {mem['mape']:.2f}%")
        
        # ë§¤ì¹­ ë°©ë²•ë³„ ë¹„êµ
        print(f"\n   ğŸ“Œ ë§¤ì¹­ ë°©ë²•ë³„ MAE ë¹„êµ:")
        
        if 'cpu_exact_match' in metrics:
            print(f"      â€¢ Exact Match CPU: {metrics['cpu_exact_match']['mae']*100:.2f}%")
        if 'cpu_global_average' in metrics:
            print(f"      â€¢ Global Avg CPU: {metrics['cpu_global_average']['mae']*100:.2f}%")
        if 'memory_exact_match' in metrics:
            print(f"      â€¢ Exact Match Memory: {metrics['memory_exact_match']['mae']*100:.2f}%")
        if 'memory_global_average' in metrics:
            print(f"      â€¢ Global Avg Memory: {metrics['memory_global_average']['mae']*100:.2f}%")
        
        # í•´ì„
        print(f"\n   ğŸ“Œ í•´ì„:")
        
        avg_mae = (cpu['mae'] + mem['mae']) / 2 * 100
        
        if avg_mae < 5:
            print(f"      ğŸ‰ ë§¤ìš° ìš°ìˆ˜í•œ ì¶”ì • ì •í™•ë„! (í‰ê·  ì˜¤ì°¨ < 5%)")
        elif avg_mae < 10:
            print(f"      âœ… ì–‘í˜¸í•œ ì¶”ì • ì •í™•ë„ (í‰ê·  ì˜¤ì°¨ 5-10%)")
        elif avg_mae < 15:
            print(f"      âš ï¸ ë³´í†µ ìˆ˜ì¤€ì˜ ì¶”ì • ì •í™•ë„ (í‰ê·  ì˜¤ì°¨ 10-15%)")
        else:
            print(f"      âŒ ê°œì„  í•„ìš” (í‰ê·  ì˜¤ì°¨ > 15%)")
        
        print(f"\n{'='*100}")
    
    
    def _convert_to_serializable(self, obj):
        """
        numpy/pandas íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        
        Args:
            obj: ë³€í™˜í•  ê°ì²´
        
        Returns:
            JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°ì²´
        """
        # DataFrameì€ ìŠ¤í‚µ (ë³„ë„ CSVë¡œ ì €ì¥)
        if isinstance(obj, pd.DataFrame):
            return None
        
        if isinstance(obj, dict):
            return {k: self._convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._convert_to_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif obj is None:
            return None
        elif isinstance(obj, (int, float, str, bool)):
            return obj
        else:
            # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
            try:
                return str(obj)
            except:
                return None
    
    
    def save(self):
        """
        ê²€ì¦ ê²°ê³¼ ì €ì¥
        
        Returns:
            self
        """
        if self.validation_results is None:
            self.print_warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return self
        
        self.print_step("ê²€ì¦ ê²°ê³¼ ì €ì¥", f"{self.output_path}")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.ensure_dir(self.output_path.parent)
        
        # DataFrame ì œì™¸í•œ ê²°ê³¼ë§Œ JSON ì €ì¥
        results_to_save = {k: v for k, v in self.validation_results.items() 
                         if k != 'df_comparison'}
        
        # numpy íƒ€ì… ë³€í™˜
        results_to_save = self._convert_to_serializable(results_to_save)
        
        with open(self.output_path, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, indent=2, ensure_ascii=False)
    
    
    def run(self):
        """
        ì „ì²´ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Returns:
            self
        """
        return (self.load()
                .split_data()
                .learn_train_patterns()
                .process()
                .process_service_level()
                .save())
    
    def get_results(self):
        """
        ê²€ì¦ ê²°ê³¼ ë°˜í™˜
        
        Returns:
            dict: ê²€ì¦ ê²°ê³¼
        """
        return self.validation_results
    
    
    def get_summary_for_paper(self):
        """
        ë…¼ë¬¸ìš© ìš”ì•½ í†µê³„
        
        Returns:
            dict: ë…¼ë¬¸ì— ë„£ì„ í•µì‹¬ ì§€í‘œ
        """
        if self.validation_results is None:
            return {}
        
        metrics = self.validation_results
        
        return {
            'test_samples': metrics['total_samples'],
            'exact_match_ratio': metrics['exact_match_ratio'],
            'cpu_mae_percent': metrics['cpu']['mae'] * 100,
            'cpu_rmse_percent': metrics['cpu']['rmse'] * 100,
            'cpu_r2': metrics['cpu']['r2'],
            'memory_mae_percent': metrics['memory']['mae'] * 100,
            'memory_rmse_percent': metrics['memory']['rmse'] * 100,
            'memory_r2': metrics['memory']['r2'],
            'avg_mae_percent': (metrics['cpu']['mae'] + metrics['memory']['mae']) / 2 * 100,
            'avg_r2': (metrics['cpu']['r2'] + metrics['memory']['r2']) / 2
        }


# ==================== ë©”ì¸ ì‹¤í–‰ ====================
if __name__ == "__main__":
    
    print("\nğŸš€ Transfer Learning ê²€ì¦ ì‹œì‘")
    print("="*100)
    
    validator = TransferLearningValidator('config/focus_config.yaml')
    validator.run()
    
    # ë…¼ë¬¸ìš© ìš”ì•½
    summary = validator.get_summary_for_paper()
    
    print(f"\nğŸ“ ë…¼ë¬¸ìš© ìš”ì•½:")
    print(f"   â€¢ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {summary.get('test_samples', 'N/A'):,}ê±´")
    print(f"   â€¢ Exact Match ë¹„ìœ¨: {summary.get('exact_match_ratio', 'N/A'):.1f}%")
    print(f"   â€¢ CPU MAE: {summary.get('cpu_mae_percent', 'N/A'):.2f}%")
    print(f"   â€¢ Memory MAE: {summary.get('memory_mae_percent', 'N/A'):.2f}%")
    print(f"   â€¢ í‰ê·  RÂ² Score: {summary.get('avg_r2', 'N/A'):.4f}")
    
    print("\nâœ… ê²€ì¦ ì™„ë£Œ!")