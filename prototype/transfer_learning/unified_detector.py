# -*- coding: utf-8 -*-
"""
í†µí•© ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ê¸° (Unified Over-Provisioning Detector)

============================================================
í•µì‹¬ ë¡œì§:
============================================================
GCP: AvgCPUUsage/AvgMemoryUsage ì§ì ‘ ë¹„êµ (< 30% â†’ ê³¼ë‹¤)
AWS: ML Classifier ì˜ˆì¸¡ ë“±ê¸‰ ì‚¬ìš© (Low â†’ ê³¼ë‹¤)

============================================================
ì…ë ¥: resource_grouped.csv (ProviderNameìœ¼ë¡œ GCP/AWS êµ¬ë¶„)
ì¶œë ¥: unified_overprovisioned.csv
============================================================

Author: Lily
Date: 2025-01
Purpose: ì„ì‚¬ ë…¼ë¬¸ - LLM ê¸°ë°˜ í´ë¼ìš°ë“œ FinOps ìë™í™” ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ
"""

import pandas as pd
import numpy as np
import yaml
import json
import joblib
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

import sys

# ============================================================
# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
# ============================================================
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'data_processing'))

try:
    from pipeline_base import PipelineBase
except ImportError:
    # PipelineBase ì—†ì„ ê²½ìš° ê¸°ë³¸ êµ¬í˜„
    class PipelineBase:
        def __init__(self, config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        
        def print_step(self, msg, detail=""):
            print(f"\n{'='*80}")
            print(f"ğŸ“Œ {msg}")
            if detail:
                print(f"   {detail}")
            print(f"{'='*80}")
        
        def print_success(self, msg):
            print(f"   âœ… {msg}")
        
        def print_warning(self, msg):
            print(f"   âš ï¸ {msg}")
        
        def print_error(self, msg):
            print(f"   âŒ {msg}")
        
        def ensure_dir(self, path):
            Path(path).mkdir(parents=True, exist_ok=True)


class UnifiedOverProvisioningDetector(PipelineBase):
    """
    í†µí•© ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ê¸°
    
    ============================================================
    íƒì§€ ë°©ë²•:
    ============================================================
    
    [GCP] ì§ì ‘ ì„ê³„ê°’ ë¹„êµ
        - AvgCPUUsage < 30% â†’ CPU ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹
        - AvgMemoryUsage < 30% â†’ Memory ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹
        - ì •í™•ë„: 100% (ì‹¤ì œ ì‚¬ìš©ë¥  ë°ì´í„° ìˆìŒ)
    
    [AWS] ML Classification ê¸°ë°˜
        - Transfer Learningìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
        - PredictedCPUClass == 'Low' â†’ CPU ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹
        - PredictedMemoryClass == 'Low' â†’ Memory ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹
        - ì •í™•ë„: 97% (ML ëª¨ë¸)
    
    ============================================================
    ì¶œë ¥ ì»¬ëŸ¼:
    ============================================================
    - ResourceId: ë¦¬ì†ŒìŠ¤ ì‹ë³„ì
    - ProviderName: GCP / AWS
    - ServiceName: ì„œë¹„ìŠ¤ëª…
    - DetectionMethod: 'Direct' (GCP) / 'ML_Classification' (AWS)
    - CPUStatus: 'OverProvisioned' / 'Normal'
    - MemoryStatus: 'OverProvisioned' / 'Normal'
    - CPUValue: ì‹¤ì œê°’(GCP) / ì˜ˆì¸¡ë“±ê¸‰(AWS)
    - MemoryValue: ì‹¤ì œê°’(GCP) / ì˜ˆì¸¡ë“±ê¸‰(AWS)
    - TotalHourlyCost: ì‹œê°„ë‹¹ ë¹„ìš©
    - PotentialSavings: ì˜ˆìƒ ì ˆê°ì•¡
    """

    # ============================================================
    # Compute ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ (ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ ëŒ€ìƒ)
    # ============================================================
    COMPUTE_KEYWORDS = [
        'compute engine', 'cloud run', 'app engine', 'cloud functions',
        'kubernetes engine', 'gke',
        'ec2', 'elastic compute', 'lambda', 'ecs', 'fargate', 'eks',
        'elastic beanstalk',
        'compute', 'vm', 'instance', 'container', 'function'
    ]

    # ì—°ì† ì‹œê°„ ì„ê³„ê°’ (24ì‹œê°„)
    MIN_CONSECUTIVE_HOURS = 24

    # ============================================================
    # ì„œë¹„ìŠ¤ â†’ UnifiedCategory ë§¤í•‘ (ML ì˜ˆì¸¡ìš©)
    # ============================================================
    SERVICE_CATEGORY_MAP = {
        # Compute
        'compute engine': 'Compute', 'ec2': 'Compute', 'amazon ec2': 'Compute',
        'cloud functions': 'Compute', 'lambda': 'Compute', 'aws lambda': 'Compute',
        'cloud run': 'Compute', 'ecs': 'Compute', 'fargate': 'Compute',
        'app engine': 'Compute', 'elastic beanstalk': 'Compute',
        
        # Container
        'kubernetes engine': 'Container', 'gke': 'Container',
        'eks': 'Container', 'amazon eks': 'Container',
        
        # Database
        'cloud sql': 'Database', 'rds': 'Database', 'amazon rds': 'Database',
        'aurora': 'Database', 'dynamodb': 'Database', 'bigtable': 'Database',
        'firestore': 'Database', 'elasticache': 'Database', 'redshift': 'Database',
        
        # Storage
        'cloud storage': 'Storage', 's3': 'Storage', 'amazon s3': 'Storage',
        'persistent disk': 'Storage', 'ebs': 'Storage', 'efs': 'Storage',
        
        # Analytics
        'bigquery': 'Analytics', 'athena': 'Analytics', 'dataproc': 'Analytics',
        'emr': 'Analytics', 'kinesis': 'Analytics', 'glue': 'Analytics',
        
        # AI/ML
        'vertex ai': 'AI_ML', 'sagemaker': 'AI_ML', 'automl': 'AI_ML',
        
        # Networking
        'vpc': 'Networking', 'cloud load balancing': 'Networking',
        'elb': 'Networking', 'cloudfront': 'Networking', 'cloud cdn': 'Networking',
        
        # Monitoring
        'cloud monitoring': 'Monitoring', 'cloudwatch': 'Monitoring',
        
        # Security
        'iam': 'Security', 'kms': 'Security', 'waf': 'Security',
        
        # Messaging
        'pub/sub': 'Messaging', 'sns': 'Messaging', 'sqs': 'Messaging',
    }
    
    
    def __init__(self, config_path='config/focus_config.yaml'):
        """
        ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        super().__init__(config_path)
        
        # ============================================================
        # ê²½ë¡œ ì„¤ì • (config ê¸°ë°˜)
        # ============================================================
        data_config = self.config['data']
        self.input_path = Path(data_config['resource_grouped_output'])
        self.output_path = Path('results/transfer_learning/unified_overprovisioned.csv')
        
        # ML ëª¨ë¸ ê²½ë¡œ
        self.model_dir = Path('results/transfer_learning/models/classifier')
        
        # ============================================================
        # ì„ê³„ê°’ ì„¤ì • (config ê¸°ë°˜)
        # ============================================================
        thresholds = self.config['thresholds']['over_provisioning']
        self.cpu_threshold = thresholds['cpu_threshold']      # 0.30 (30%)
        self.memory_threshold = thresholds['memory_threshold']  # 0.30 (30%)
        
        # ============================================================
        # ë°ì´í„° & ëª¨ë¸
        # ============================================================
        self.df = None              # ì „ì²´ ë°ì´í„°
        self.df_gcp = None          # GCP ë°ì´í„°
        self.df_aws = None          # AWS ë°ì´í„°
        
        self.df_gcp_result = None   # GCP íƒì§€ ê²°ê³¼
        self.df_aws_result = None   # AWS íƒì§€ ê²°ê³¼
        self.df_unified = None      # í†µí•© ê²°ê³¼
        
        # ML ëª¨ë¸ (AWSìš©)
        self.cpu_model = None
        self.memory_model = None
        self.label_encoders = {}
        self.scaler = None
        self.class_encoder = None
        
        # í†µê³„
        self.stats = {
            'gcp': {'total': 0, 'over_provisioned': 0},
            'aws': {'total': 0, 'over_provisioned': 0}
        }
    
    def _is_compute_service(self, service_name):
        """
        Compute ì„œë¹„ìŠ¤ì¸ì§€ í™•ì¸
        
        Args:
            service_name: ì„œë¹„ìŠ¤ëª…
        
        Returns:
            bool: Compute ì„œë¹„ìŠ¤ ì—¬ë¶€
        """
        if pd.isna(service_name):
            return False
        
        service_lower = str(service_name).lower()
        
        for keyword in self.COMPUTE_KEYWORDS:
            if keyword in service_lower:
                return True
        
        return False
    
    def _find_consecutive_hours(self, df, flag_col):
        """
        ì—°ì† Trueì¸ ìµœëŒ€ ì‹œê°„ ì°¾ê¸°
        
        Args:
            df: ì‹œê°„ìˆœ ì •ë ¬ëœ DataFrame
            flag_col: ì²´í¬í•  boolean ì»¬ëŸ¼ëª…
        
        Returns:
            int: ìµœëŒ€ ì—°ì† ì‹œê°„
        """
        if flag_col not in df.columns or len(df) == 0:
            return 0
        
        flags = df[flag_col].values
        max_consecutive = 0
        current_consecutive = 0
        
        for flag in flags:
            if flag:
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 0
        
        return max_consecutive
    
    def load(self):
        """
        ë°ì´í„° ë° ML ëª¨ë¸ ë¡œë“œ
        
        Returns:
            self
        """
        self.print_step("ë°ì´í„° ë¡œë“œ", f"{self.input_path}")
        
        # ============================================================
        # 1. resource_grouped.csv ë¡œë“œ
        # ============================================================
        if not self.input_path.exists():
            self.print_error(f"íŒŒì¼ ì—†ìŒ: {self.input_path}")
            raise FileNotFoundError(f"{self.input_path}")
        
        self.df = pd.read_csv(self.input_path)
        self.print_success(f"ë°ì´í„° ë¡œë“œ: {len(self.df):,}ê±´")
        
        # ============================================================
        # 2. GCP/AWS ë¶„ë¦¬
        # ============================================================
        self._separate_by_provider()
        
        # ============================================================
        # 3. ML ëª¨ë¸ ë¡œë“œ (AWS ì˜ˆì¸¡ìš©)
        # ============================================================
        if len(self.df_aws) > 0:
            self._load_ml_models()
        
        return self
    
    
    def _separate_by_provider(self):
        """
        ProviderNameìœ¼ë¡œ GCP/AWS ë°ì´í„° ë¶„ë¦¬ + Computeë§Œ í•„í„°ë§
        """
        print(f"\n   ğŸ”„ í´ë¼ìš°ë“œ ì œê³µìë³„ ë¶„ë¦¬...")
        
        # ProviderName ì»¬ëŸ¼ í™•ì¸
        if 'ProviderName' not in self.df.columns:
            self.print_warning("ProviderName ì»¬ëŸ¼ ì—†ìŒ - ì „ì²´ë¥¼ GCPë¡œ ì²˜ë¦¬")
            self.df_gcp = self.df.copy()
            self.df_aws = pd.DataFrame()
            return
        
        # Compute ì„œë¹„ìŠ¤ë§Œ í•„í„°ë§
        self.df['IsCompute'] = self.df['ServiceName'].apply(self._is_compute_service)
        df_compute = self.df[self.df['IsCompute']].copy()
        
        compute_pct = len(df_compute) / len(self.df) * 100 if len(self.df) > 0 else 0
        print(f"   ğŸ“Š Compute í•„í„°ë§: {len(df_compute):,}ê±´ / {len(self.df):,}ê±´ ({compute_pct:.1f}%)")
        
        # GCP/AWS ë¶„ë¦¬
        gcp_mask = df_compute['ProviderName'].str.lower().str.contains('gcp|google', na=False)
        aws_mask = df_compute['ProviderName'].str.lower().str.contains('aws|amazon', na=False)
        
        self.df_gcp = df_compute[gcp_mask].copy()
        self.df_aws = df_compute[aws_mask].copy()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['gcp']['total'] = len(self.df_gcp)
        self.stats['aws']['total'] = len(self.df_aws)
        
        print(f"   âœ… GCP: {len(self.df_gcp):,}ê±´")
        print(f"   âœ… AWS: {len(self.df_aws):,}ê±´")
        
        # ê¸°íƒ€ ë°ì´í„° ê²½ê³ 
        other_count = len(self.df) - len(self.df_gcp) - len(self.df_aws)
        if other_count > 0:
            self.print_warning(f"ê¸°íƒ€ Provider: {other_count:,}ê±´ (ì œì™¸ë¨)")
    
    
    def _load_ml_models(self):
        """
        AWS ì˜ˆì¸¡ìš© ML ëª¨ë¸ ë¡œë“œ
        """
        print(f"\n   ğŸ”„ ML ëª¨ë¸ ë¡œë“œ...")
        
        # ëª¨ë¸ íŒŒì¼ í™•ì¸
        cpu_model_path = self.model_dir / 'cpu_classifier.joblib'
        memory_model_path = self.model_dir / 'memory_classifier.joblib'
        
        if not cpu_model_path.exists():
            self.print_error(f"CPU ëª¨ë¸ ì—†ìŒ: {cpu_model_path}")
            self.print_warning("ML ëª¨ë¸ ì—†ìŒ - AWS ë°ì´í„°ëŠ” ì²˜ë¦¬ ë¶ˆê°€")
            self.print_warning("ë¨¼ì € ml_usage_classifier.py ì‹¤í–‰ í•„ìš”")
            return
        
        # ëª¨ë¸ ë¡œë“œ
        self.cpu_model = joblib.load(cpu_model_path)
        self.memory_model = joblib.load(memory_model_path)
        self.label_encoders = joblib.load(self.model_dir / 'label_encoders.joblib')
        self.scaler = joblib.load(self.model_dir / 'scaler.joblib')
        self.class_encoder = joblib.load(self.model_dir / 'class_encoder.joblib')
        
        self.print_success("ML ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"      â€¢ CPU Classifier: âœ…")
        print(f"      â€¢ Memory Classifier: âœ…")
        print(f"      â€¢ Label Encoders: âœ…")
        print(f"      â€¢ Scaler: âœ…")
    
    
    def process(self):
        """
        ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ ì‹¤í–‰
        
        Returns:
            self
        """
        self.print_step("ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€")
        
        results = []
        
        # ============================================================
        # 1. GCP ì§ì ‘ íƒì§€
        # ============================================================
        if len(self.df_gcp) > 0:
            self.df_gcp_result = self._detect_gcp()
            if len(self.df_gcp_result) > 0:
                results.append(self.df_gcp_result)
        
        # ============================================================
        # 2. AWS ML ê¸°ë°˜ íƒì§€
        # ============================================================
        if len(self.df_aws) > 0 and self.cpu_model is not None:
            self.df_aws_result = self._detect_aws()
            if len(self.df_aws_result) > 0:
                results.append(self.df_aws_result)
        elif len(self.df_aws) > 0:
            self.print_warning("AWS ë°ì´í„° ìˆìœ¼ë‚˜ ML ëª¨ë¸ ì—†ìŒ - ìŠ¤í‚µ")
        
        # ============================================================
        # 3. ê²°ê³¼ í†µí•©
        # ============================================================
        if results:
            self.df_unified = pd.concat(results, ignore_index=True)
        else:
            self.df_unified = pd.DataFrame()
        
        # í†µê³„ ì¶œë ¥
        self._print_summary()
        
        return self
    
    
    def _detect_gcp(self):
        """
        GCP ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ (ì—°ì† 24ì‹œê°„ ì²´í¬)
        
        ì¡°ê±´:
        - AvgCPUUsage < 30% ì—°ì† 24ì‹œê°„ â†’ CPU ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹
        - AvgMemoryUsage < 30% ì—°ì† 24ì‹œê°„ â†’ Memory ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹
        """
        print(f"\n   ğŸ” [GCP] ì—°ì† {self.MIN_CONSECUTIVE_HOURS}ì‹œê°„ ì €ì‚¬ìš©ë¥  ì²´í¬...")
        print(f"      â€¢ CPU ì„ê³„ê°’: < {self.cpu_threshold*100:.0f}%")
        print(f"      â€¢ Memory ì„ê³„ê°’: < {self.memory_threshold*100:.0f}%")
        
        df = self.df_gcp.copy()
        
        cpu_col = 'AvgCPUUsage'
        mem_col = 'AvgMemoryUsage'
        
        if cpu_col not in df.columns:
            self.print_warning(f"GCP ë°ì´í„°ì— {cpu_col} ì—†ìŒ")
            return pd.DataFrame()
        
        # íƒ€ì… ë³€í™˜
        df['HourlyTimestamp'] = pd.to_datetime(df['HourlyTimestamp'])
        df[cpu_col] = pd.to_numeric(df[cpu_col], errors='coerce').fillna(0)
        df[mem_col] = pd.to_numeric(df[mem_col], errors='coerce').fillna(0)
        
        # ì €ì‚¬ìš©ë¥  í”Œë˜ê·¸
        df['CPULow'] = df[cpu_col] < self.cpu_threshold
        df['MemoryLow'] = df[mem_col] < self.memory_threshold
        
        # ResourceIdë³„ ì—°ì† ì €ì‚¬ìš©ë¥  ì²´í¬
        over_provisioned = []
        print(f"      ğŸ“Š GCP ë¦¬ì†ŒìŠ¤ ìˆ˜: {df['ResourceId'].nunique():,}ê°œ")
        
        grouped = df.sort_values('HourlyTimestamp').groupby('ResourceId')

        for i, (resource_id, resource_df) in enumerate(grouped):
            if (i + 1) % 1000 == 0:
                print(f"         ì§„í–‰: {i+1:,}ê°œ ì²˜ë¦¬...")
            
            cpu_consecutive = self._find_consecutive_hours(resource_df, 'CPULow')
            mem_consecutive = self._find_consecutive_hours(resource_df, 'MemoryLow')
            
            if cpu_consecutive >= self.MIN_CONSECUTIVE_HOURS or mem_consecutive >= self.MIN_CONSECUTIVE_HOURS:
                last_record = resource_df.iloc[-1].to_dict()
                last_record['ConsecutiveCPUHours'] = cpu_consecutive
                last_record['ConsecutiveMemoryHours'] = mem_consecutive
                last_record['DetectionMethod'] = 'Direct_Consecutive'
                last_record['CPUStatus'] = 'OverProvisioned' if cpu_consecutive >= self.MIN_CONSECUTIVE_HOURS else 'Normal'
                last_record['MemoryStatus'] = 'OverProvisioned' if mem_consecutive >= self.MIN_CONSECUTIVE_HOURS else 'Normal'
                last_record['CPUValue'] = f"{resource_df[cpu_col].mean()*100:.1f}%"
                last_record['MemoryValue'] = f"{resource_df[mem_col].mean()*100:.1f}%"
                
                over_provisioned.append(last_record)
        
        if over_provisioned:
            result = pd.DataFrame(over_provisioned)
            
            # ë¹„ìš© ê³„ì‚°
            if 'TotalHourlyCost' in result.columns:
                result['TotalHourlyCost'] = pd.to_numeric(result['TotalHourlyCost'], errors='coerce').fillna(0)
                result['WasteRatio'] = 0.7  # ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ = ì•½ 70% ë‚­ë¹„ ê°€ì •
                result['PotentialSavings'] = result['TotalHourlyCost'] * result['WasteRatio']
            
            self.stats['gcp']['over_provisioned'] = len(result)
            print(f"      âœ… íƒì§€: {len(result):,}ê±´")
            return result
        else:
            print(f"      â„¹ï¸ ì—°ì† {self.MIN_CONSECUTIVE_HOURS}ì‹œê°„ ì´ìƒ ì €ì‚¬ìš©ë¥  ì—†ìŒ")
            return pd.DataFrame()
    
    
    def _detect_aws(self):
        """
        AWS ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ (ML + ì—°ì† 24ì‹œê°„ ì²´í¬)
        
        ì¡°ê±´:
        - PredictedCPUClass = 'Low' ì—°ì† 24ì‹œê°„ â†’ CPU ê³¼ë‹¤
        - PredictedMemoryClass = 'Low' ì—°ì† 24ì‹œê°„ â†’ Memory ê³¼ë‹¤
        """
        print(f"\n   ğŸ” [AWS] ML ì˜ˆì¸¡ + ì—°ì† {self.MIN_CONSECUTIVE_HOURS}ì‹œê°„ ì²´í¬...")
        
        df = self.df_aws.copy()
        
        # Feature ì¤€ë¹„
        df = self._prepare_features(df)
        
        # ML ì˜ˆì¸¡
        try:
            X = self._encode_features(df)
            if X is None:
                return pd.DataFrame()
            
            cpu_pred = self.class_encoder.inverse_transform(self.cpu_model.predict(X))
            mem_pred = self.class_encoder.inverse_transform(self.memory_model.predict(X))
            
            df['PredictedCPUClass'] = cpu_pred
            df['PredictedMemoryClass'] = mem_pred
            
        except Exception as e:
            self.print_error(f"ML ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
        
        # íƒ€ì… ë³€í™˜
        df['HourlyTimestamp'] = pd.to_datetime(df['HourlyTimestamp'])
        
        # Low ë“±ê¸‰ í”Œë˜ê·¸
        df['CPULow'] = df['PredictedCPUClass'] == 'Low'
        df['MemoryLow'] = df['PredictedMemoryClass'] == 'Low'
        
        # ResourceIdë³„ ì—°ì† Low ì²´í¬
        over_provisioned = []

        unique_resources = df['ResourceId'].unique()
        total_resources = len(unique_resources)
        print(f"      ğŸ“Š AWS ë¦¬ì†ŒìŠ¤ ìˆ˜: {total_resources:,}ê°œ")
        grouped = df.sort_values('HourlyTimestamp').groupby('ResourceId')

        for i, (resource_id, resource_df) in enumerate(grouped):
            if (i + 1) % 10000 == 0:
                print(f"         ì§„í–‰: {i+1:,}ê°œ ì²˜ë¦¬...")
            
            cpu_consecutive = self._find_consecutive_hours(resource_df, 'CPULow')
            mem_consecutive = self._find_consecutive_hours(resource_df, 'MemoryLow')
            
            if cpu_consecutive >= self.MIN_CONSECUTIVE_HOURS or mem_consecutive >= self.MIN_CONSECUTIVE_HOURS:
                last_record = resource_df.iloc[-1].to_dict()
                last_record['ConsecutiveCPUHours'] = cpu_consecutive
                last_record['ConsecutiveMemoryHours'] = mem_consecutive
                last_record['DetectionMethod'] = 'ML_Consecutive'
                last_record['CPUStatus'] = 'OverProvisioned' if cpu_consecutive >= self.MIN_CONSECUTIVE_HOURS else 'Normal'
                last_record['MemoryStatus'] = 'OverProvisioned' if mem_consecutive >= self.MIN_CONSECUTIVE_HOURS else 'Normal'
                last_record['CPUValue'] = 'Low (ML)'
                last_record['MemoryValue'] = 'Low (ML)'
                
                over_provisioned.append(last_record)
        
        if over_provisioned:
            result = pd.DataFrame(over_provisioned)
            
            if 'TotalHourlyCost' in result.columns:
                result['TotalHourlyCost'] = pd.to_numeric(result['TotalHourlyCost'], errors='coerce').fillna(0)
                result['WasteRatio'] = 0.5
                result['PotentialSavings'] = result['TotalHourlyCost'] * result['WasteRatio']
            
            self.stats['aws']['over_provisioned'] = len(result)
            print(f"      âœ… íƒì§€: {len(result):,}ê±´")
            return result
        else:
            print(f"      â„¹ï¸ ì—°ì† {self.MIN_CONSECUTIVE_HOURS}ì‹œê°„ ì´ìƒ Low ì—†ìŒ")
            return pd.DataFrame()
    
    
    def _prepare_features(self, df):
        """
        AWS ë°ì´í„° Feature ì¤€ë¹„
        """
        # ServiceName â†’ UnifiedCategory
        if 'ServiceName' in df.columns:
            df['UnifiedCategory'] = df['ServiceName'].apply(self._map_to_category)
        else:
            df['UnifiedCategory'] = 'Other'
        
        # LogCost
        cost_col = 'TotalHourlyCost'
        if cost_col in df.columns:
            df['LogCost'] = np.log1p(
                pd.to_numeric(df[cost_col], errors='coerce').fillna(0)
            )
        else:
            df['LogCost'] = 0
        
        # HourOfDay, DayOfWeek
        if 'HourlyTimestamp' in df.columns:
            df['HourlyTimestamp'] = pd.to_datetime(df['HourlyTimestamp'], errors='coerce')
            df['HourOfDay'] = df['HourlyTimestamp'].dt.hour.fillna(12).astype(int)
            df['DayOfWeek'] = df['HourlyTimestamp'].dt.dayofweek.fillna(3).astype(int)
        else:
            df['HourOfDay'] = 12
            df['DayOfWeek'] = 3
        
        # ResourceType ê¸°ë³¸ê°’
        if 'ResourceType' not in df.columns:
            df['ResourceType'] = 'Unknown'
        
        return df
    
    
    def _map_to_category(self, service_name):
        """
        ì„œë¹„ìŠ¤ëª… â†’ UnifiedCategory ë§¤í•‘
        """
        if pd.isna(service_name):
            return 'Other'
        
        service_lower = str(service_name).lower()
        
        for keyword, category in self.SERVICE_CATEGORY_MAP.items():
            if keyword in service_lower:
                return category
        
        return 'Other'
    
    
    def _encode_features(self, df):
        """
        Feature ì¸ì½”ë”© (í•™ìŠµëœ ì¸ì½”ë” ì‚¬ìš©)
        """
        try:
            encoded_data = []
            
            # Categorical
            categorical_cols = ['UnifiedCategory', 'ResourceType']
            for col in categorical_cols:
                if col not in self.label_encoders:
                    continue
                
                encoder = self.label_encoders[col]
                known_classes = set(encoder.classes_)
                
                values = df[col].fillna('Unknown').astype(str)
                values = values.apply(lambda x: x if x in known_classes else 'Unknown')
                
                encoded = encoder.transform(values)
                encoded_data.append(encoded.reshape(-1, 1))
            
            # Numerical
            numerical_cols = ['LogCost', 'HourOfDay', 'DayOfWeek']
            numerical_data = df[numerical_cols].fillna(0).values
            numerical_scaled = self.scaler.transform(numerical_data)
            encoded_data.append(numerical_scaled)
            
            return np.hstack(encoded_data)
        
        except Exception as e:
            self.print_error(f"Feature ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return None
    
    
    def _print_summary(self):
        """
        íƒì§€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*80}")
        
        # GCP í†µê³„
        gcp_total = self.stats['gcp']['total']
        gcp_over = self.stats['gcp']['over_provisioned']
        gcp_pct = (gcp_over / gcp_total * 100) if gcp_total > 0 else 0
        
        print(f"\n   [GCP] ì§ì ‘ íƒì§€ (ì„ê³„ê°’ < 30%)")
        print(f"      â€¢ ì „ì²´: {gcp_total:,}ê±´")
        print(f"      â€¢ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {gcp_over:,}ê±´ ({gcp_pct:.1f}%)")
        
        # AWS í†µê³„
        aws_total = self.stats['aws']['total']
        aws_over = self.stats['aws']['over_provisioned']
        aws_pct = (aws_over / aws_total * 100) if aws_total > 0 else 0
        
        print(f"\n   [AWS] ML Classification (Low ë“±ê¸‰)")
        print(f"      â€¢ ì „ì²´: {aws_total:,}ê±´")
        print(f"      â€¢ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {aws_over:,}ê±´ ({aws_pct:.1f}%)")
        
        # í†µí•© í†µê³„
        total = gcp_total + aws_total
        total_over = gcp_over + aws_over
        total_pct = (total_over / total * 100) if total > 0 else 0
        
        print(f"\n   [í†µí•©]")
        print(f"      â€¢ ì „ì²´: {total:,}ê±´")
        print(f"      â€¢ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {total_over:,}ê±´ ({total_pct:.1f}%)")
        
        # ì˜ˆìƒ ì ˆê°ì•¡
        if self.df_unified is not None and 'PotentialSavings' in self.df_unified.columns:
            savings = self.df_unified['PotentialSavings'].sum()
            print(f"\n   ğŸ’° ì˜ˆìƒ ì ˆê°ì•¡:")
            print(f"      â€¢ ì‹œê°„ë‹¹: ${savings:,.2f}")
            print(f"      â€¢ ì›”ê°„: ${savings * 24 * 30:,.2f}")
            print(f"      â€¢ ì—°ê°„: ${savings * 24 * 365:,.2f}")
        
        print(f"\n{'='*80}")
    
    
    def save(self):
        """
        ê²°ê³¼ ì €ì¥
        
        Returns:
            self
        """
        self.print_step("ê²°ê³¼ ì €ì¥")
        
        if self.df_unified is None or len(self.df_unified) == 0:
            self.print_warning("ì €ì¥í•  ê²°ê³¼ ì—†ìŒ")
            return self
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.ensure_dir(self.output_path.parent)
        
        # ì¶œë ¥ ì»¬ëŸ¼ ì„ íƒ
        output_cols = [
            'ResourceId', 'ProviderName', 'ServiceName', 'ResourceType',
            'DetectionMethod', 'CPUStatus', 'MemoryStatus',
            'CPUValue', 'MemoryValue',
            'TotalHourlyCost', 'WasteRatio', 'PotentialSavings'
        ]
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_cols = [col for col in output_cols if col in self.df_unified.columns]
        df_output = self.df_unified[available_cols]
        
        # CSV ì €ì¥
        df_output.to_csv(self.output_path, index=False, encoding='utf-8-sig')
        
        self.print_success(f"ì €ì¥ ì™„ë£Œ: {self.output_path}")
        print(f"      â€¢ ë ˆì½”ë“œ: {len(df_output):,}ê±´")
        
        # GCP/AWS ë³„ë„ ì €ì¥
        if self.df_gcp_result is not None and len(self.df_gcp_result) > 0:
            gcp_path = self.output_path.parent / 'gcp_overprovisioned.csv'
            self.df_gcp_result.to_csv(gcp_path, index=False, encoding='utf-8-sig')
            print(f"      â€¢ GCP: {gcp_path}")
        
        if self.df_aws_result is not None and len(self.df_aws_result) > 0:
            aws_path = self.output_path.parent / 'aws_overprovisioned.csv'
            self.df_aws_result.to_csv(aws_path, index=False, encoding='utf-8-sig')
            print(f"      â€¢ AWS: {aws_path}")
        
        # í†µê³„ JSON ì €ì¥
        stats_path = self.output_path.parent / 'detection_stats.json'
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, ensure_ascii=False)
        print(f"      â€¢ í†µê³„: {stats_path}")
        
        return self
    
    
    def run(self):
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Returns:
            self
        """
        return self.load().process().save()
    
    
    def get_results(self):
        """
        ê²°ê³¼ ë°˜í™˜
        
        Returns:
            tuple: (í†µí•© ê²°ê³¼ DataFrame, í†µê³„ ë”•ì…”ë„ˆë¦¬)
        """
        return (self.df_unified, self.stats)


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    
    print("\n" + "="*80)
    print("ğŸš€ í†µí•© ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ê¸°")
    print("="*80)
    print("ğŸ“Œ íƒì§€ ë°©ë²•:")
    print("   â€¢ GCP: ì§ì ‘ ì„ê³„ê°’ ë¹„êµ (AvgCPU/Memory < 30%)")
    print("   â€¢ AWS: ML Classification (Low ë“±ê¸‰ = ê³¼ë‹¤)")
    print("="*80)
    
    detector = UnifiedOverProvisioningDetector('config/focus_config.yaml')
    detector.run()
    
    df_result, stats = detector.get_results()
    
    print(f"\nâœ… ì™„ë£Œ!")
    if df_result is not None:
        print(f"   ì´ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {len(df_result):,}ê±´")