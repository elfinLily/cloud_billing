# -*- coding: utf-8 -*-
"""
ML ê¸°ë°˜ ì‚¬ìš©ë¥  ì˜ˆì¸¡ ëª¨ë¸

GCP ë°ì´í„°ë¡œ í•™ìŠµí•˜ì—¬ AWS ë¦¬ì†ŒìŠ¤ì˜ CPU/Memory ì‚¬ìš©ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
RandomForest íšŒê·€ ëª¨ë¸ ì‚¬ìš©
"""

import pandas as pd
import numpy as np
import yaml
import json
import joblib
from pathlib import Path
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'data_processing'))

from pipeline_base import PipelineBase


class MLUsagePredictor(PipelineBase):
    """
    ML ê¸°ë°˜ ì‚¬ìš©ë¥  ì˜ˆì¸¡ í´ë˜ìŠ¤
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. GCP ë°ì´í„°ì—ì„œ Feature ì¶”ì¶œ
    2. RandomForest ëª¨ë¸ í•™ìŠµ
    3. AWS ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì‚¬ìš©ë¥  ì˜ˆì¸¡
    4. ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€
    """
    
    def __init__(self, config_path='config/focus_config.yaml'):
        """
        ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        super().__init__(config_path)
        
        # ê²½ë¡œ ì„¤ì •
        data_config = self.config['data']
        self.gcp_data_path = Path(data_config['gcp_raw_path'])
        self.aws_data_path = Path(data_config['aws_focus_output'])
        self.model_output_dir = Path('results/transfer_learning/models')
        self.result_output_path = Path('results/transfer_learning/ml_predictions.csv')
        
        # ì„ê³„ê°’
        thresholds = self.config['thresholds']['over_provisioning']
        self.cpu_threshold = thresholds['cpu_threshold']
        self.memory_threshold = thresholds['memory_threshold']
        
        # ëª¨ë¸
        self.cpu_model = None
        self.memory_model = None
        self.label_encoders = {}
        self.scaler = StandardScaler()
        
        # ë°ì´í„°
        self.df_gcp = None
        self.df_aws = None
        self.df_predictions = None
        
        # Feature ì»¬ëŸ¼
        self.feature_cols = []
        self.categorical_cols = ['ServiceCategory', 'ResourceType']
        self.numerical_cols = ['HourlyCost', 'HourOfDay', 'DayOfWeek', 'CostPerQuantity']
    
    
    def load(self):
        """
        GCP ë°ì´í„° ë¡œë“œ
        
        Returns:
            self
        """
        self.print_step("GCP í•™ìŠµ ë°ì´í„° ë¡œë”©", f"{self.gcp_data_path}")
        
        if not self.gcp_data_path.exists():
            self.print_error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.gcp_data_path}")
            raise FileNotFoundError(f"{self.gcp_data_path}")
        
        self.df_gcp = pd.read_csv(self.gcp_data_path)
        
        self.print_success("ë¡œë“œ ì™„ë£Œ")
        print(f"   ğŸ“Š ë ˆì½”ë“œ: {len(self.df_gcp):,}ê±´")
        print(f"   ğŸ“‹ ì»¬ëŸ¼: {len(self.df_gcp.columns)}ê°œ")
        
        # ì»¬ëŸ¼ í™•ì¸
        print(f"\n   ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼:")
        for col in self.df_gcp.columns[:15]:
            print(f"      â€¢ {col}")
        if len(self.df_gcp.columns) > 15:
            print(f"      ... ì™¸ {len(self.df_gcp.columns) - 15}ê°œ")
        
        return self
    
    
    def _find_columns(self):
        """
        í•„ìš”í•œ ì»¬ëŸ¼ ë§¤í•‘
        
        Returns:
            dict: ì»¬ëŸ¼ ë§¤í•‘
        """
        col_mapping = {}
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_cols = [col for col in self.df_gcp.columns 
                   if 'cpu' in col.lower() and ('usage' in col.lower() or 'utilization' in col.lower())]
        col_mapping['cpu'] = cpu_cols[0] if cpu_cols else None
        
        # Memory ì‚¬ìš©ë¥ 
        mem_cols = [col for col in self.df_gcp.columns 
                   if 'memory' in col.lower() and ('usage' in col.lower() or 'utilization' in col.lower())]
        col_mapping['memory'] = mem_cols[0] if mem_cols else None
        
        # ì„œë¹„ìŠ¤ëª…
        service_cols = [col for col in self.df_gcp.columns 
                       if 'service' in col.lower() and 'name' in col.lower()]
        col_mapping['service'] = service_cols[0] if service_cols else None
        
        # ë¹„ìš©
        cost_cols = [col for col in self.df_gcp.columns 
                   if 'cost' in col.lower() and 'round' in col.lower()]
        if not cost_cols:
            cost_cols = [col for col in self.df_gcp.columns if 'cost' in col.lower()]
        col_mapping['cost'] = cost_cols[0] if cost_cols else None
        
        # ë‚ ì§œ
        date_cols = [col for col in self.df_gcp.columns 
                   if 'date' in col.lower() or 'time' in col.lower() or 'start' in col.lower()]
        col_mapping['date'] = date_cols[0] if date_cols else None
        
        # ë‹¨ìœ„ë‹¹ ë¹„ìš©
        unit_cost_cols = [col for col in self.df_gcp.columns 
                        if 'cost' in col.lower() and 'per' in col.lower()]
        col_mapping['cost_per_unit'] = unit_cost_cols[0] if unit_cost_cols else None
        
        print(f"\n   ğŸ” ì»¬ëŸ¼ ë§¤í•‘:")
        for key, col in col_mapping.items():
            status = "âœ…" if col else "âŒ"
            print(f"      {status} {key}: {col}")
        
        return col_mapping
    
    
    def _extract_features(self, df, col_mapping, is_training=True):
        """
        Feature ì¶”ì¶œ
        
        Args:
            df: ì›ë³¸ DataFrame
            col_mapping: ì»¬ëŸ¼ ë§¤í•‘
            is_training: í•™ìŠµìš© ë°ì´í„°ì¸ì§€ ì—¬ë¶€
        
        Returns:
            DataFrame: Feature DataFrame
        """
        print(f"\n   ğŸ”§ Feature ì¶”ì¶œ ì¤‘...")
        
        features = pd.DataFrame()
        
        # 1. ServiceCategory (ì„œë¹„ìŠ¤ëª…ì—ì„œ ì¶”ì¶œ)
        if col_mapping['service']:
            features['ServiceCategory'] = df[col_mapping['service']].apply(
                self._categorize_service
            )
        else:
            features['ServiceCategory'] = 'Unknown'
        
        # 2. ResourceType (ì„œë¹„ìŠ¤ëª…ì—ì„œ ì¶”ì¶œ)
        if col_mapping['service']:
            features['ResourceType'] = df[col_mapping['service']].apply(
                self._extract_resource_type
            )
        else:
            features['ResourceType'] = 'Unknown'
        
        # 3. HourlyCost
        if col_mapping['cost']:
            features['HourlyCost'] = pd.to_numeric(df[col_mapping['cost']], errors='coerce').fillna(0)
        else:
            features['HourlyCost'] = 0
        
        # 4. HourOfDay, DayOfWeek (ë‚ ì§œì—ì„œ ì¶”ì¶œ)
        if col_mapping['date']:
            try:
                dates = pd.to_datetime(df[col_mapping['date']], errors='coerce')
                features['HourOfDay'] = dates.dt.hour.fillna(12)
                features['DayOfWeek'] = dates.dt.dayofweek.fillna(3)
            except:
                features['HourOfDay'] = 12
                features['DayOfWeek'] = 3
        else:
            features['HourOfDay'] = 12
            features['DayOfWeek'] = 3
        
        # 5. CostPerQuantity
        if col_mapping['cost_per_unit']:
            features['CostPerQuantity'] = pd.to_numeric(
                df[col_mapping['cost_per_unit']], errors='coerce'
            ).fillna(0)
        else:
            features['CostPerQuantity'] = 0
        
        # 6. Target ë³€ìˆ˜ (í•™ìŠµìš©ë§Œ)
        if is_training:
            if col_mapping['cpu']:
                cpu_vals = pd.to_numeric(df[col_mapping['cpu']], errors='coerce')
                # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                if cpu_vals.max() > 1.5:
                    cpu_vals = cpu_vals / 100.0
                features['CPUUsage'] = cpu_vals
            
            if col_mapping['memory']:
                mem_vals = pd.to_numeric(df[col_mapping['memory']], errors='coerce')
                if mem_vals.max() > 1.5:
                    mem_vals = mem_vals / 100.0
                features['MemoryUsage'] = mem_vals
        
        print(f"      âœ… Feature ì¶”ì¶œ ì™„ë£Œ: {len(features)}ê±´, {len(features.columns)}ê°œ ì»¬ëŸ¼")
        
        return features
    
    
    def _categorize_service(self, service_name):
        """
        ì„œë¹„ìŠ¤ëª…ì„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
        
        Args:
            service_name: ì„œë¹„ìŠ¤ëª…
        
        Returns:
            str: ì¹´í…Œê³ ë¦¬
        """
        if pd.isna(service_name):
            return 'Other'
        
        service_lower = str(service_name).lower()
        
        if any(kw in service_lower for kw in ['compute', 'engine', 'ec2', 'vm', 'instance']):
            return 'Compute'
        elif any(kw in service_lower for kw in ['storage', 's3', 'disk', 'bucket']):
            return 'Storage'
        elif any(kw in service_lower for kw in ['sql', 'database', 'rds', 'dynamo', 'firestore']):
            return 'Database'
        elif any(kw in service_lower for kw in ['network', 'vpc', 'load', 'cdn', 'cloudfront']):
            return 'Networking'
        elif any(kw in service_lower for kw in ['lambda', 'function', 'run', 'container']):
            return 'Serverless'
        elif any(kw in service_lower for kw in ['ai', 'ml', 'sagemaker', 'vertex']):
            return 'AI_ML'
        elif any(kw in service_lower for kw in ['monitor', 'log', 'cloudwatch', 'trace']):
            return 'Monitoring'
        elif any(kw in service_lower for kw in ['bigquery', 'analytics', 'athena', 'kinesis']):
            return 'Analytics'
        else:
            return 'Other'
    
    
    def _extract_resource_type(self, service_name):
        """
        ì„œë¹„ìŠ¤ëª…ì—ì„œ ë¦¬ì†ŒìŠ¤ íƒ€ì… ì¶”ì¶œ
        
        Args:
            service_name: ì„œë¹„ìŠ¤ëª…
        
        Returns:
            str: ë¦¬ì†ŒìŠ¤ íƒ€ì…
        """
        if pd.isna(service_name):
            return 'Other'
        
        service_lower = str(service_name).lower()
        
        if any(kw in service_lower for kw in ['vm', 'instance', 'engine']):
            return 'VM'
        elif any(kw in service_lower for kw in ['container', 'kubernetes', 'ecs', 'eks']):
            return 'Container'
        elif any(kw in service_lower for kw in ['function', 'lambda']):
            return 'Function'
        elif any(kw in service_lower for kw in ['storage', 'bucket', 's3']):
            return 'ObjectStorage'
        elif any(kw in service_lower for kw in ['disk', 'volume', 'ebs']):
            return 'BlockStorage'
        elif any(kw in service_lower for kw in ['sql', 'database']):
            return 'Database'
        else:
            return 'Other'
    
    
    def _encode_features(self, features, fit=True):
        """
        ì¹´í…Œê³ ë¦¬ Feature ì¸ì½”ë”©
        
        Args:
            features: Feature DataFrame
            fit: ì¸ì½”ë” í•™ìŠµ ì—¬ë¶€
        
        Returns:
            numpy array: ì¸ì½”ë”©ëœ Feature
        """
        df_encoded = features.copy()
        
        # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ì¸ì½”ë”©
        for col in self.categorical_cols:
            if col in df_encoded.columns:
                if fit:
                    self.label_encoders[col] = LabelEncoder()
                    df_encoded[col] = self.label_encoders[col].fit_transform(
                        df_encoded[col].astype(str)
                    )
                else:
                    # í•™ìŠµ ì‹œ ì—†ë˜ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
                    le = self.label_encoders.get(col)
                    if le:
                        df_encoded[col] = df_encoded[col].astype(str).apply(
                            lambda x: le.transform([x])[0] if x in le.classes_ 
                            else le.transform([le.classes_[0]])[0]
                        )
        
        # Feature ì»¬ëŸ¼ ì„ íƒ
        feature_cols = self.categorical_cols + self.numerical_cols
        feature_cols = [col for col in feature_cols if col in df_encoded.columns]
        
        X = df_encoded[feature_cols].values
        
        # ìˆ˜ì¹˜í˜• ì •ê·œí™”
        if fit:
            X = self.scaler.fit_transform(X)
        else:
            X = self.scaler.transform(X)
        
        self.feature_cols = feature_cols
        
        return X
    
    
    def process(self):
        """
        ML ëª¨ë¸ í•™ìŠµ
        
        Returns:
            self
        """
        self.print_step("ML ëª¨ë¸ í•™ìŠµ")
        
        # ì»¬ëŸ¼ ë§¤í•‘
        col_mapping = self._find_columns()
        
        if not col_mapping['cpu'] or not col_mapping['memory']:
            self.print_error("CPU/Memory ì‚¬ìš©ë¥  ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return self
        
        # Feature ì¶”ì¶œ
        features = self._extract_features(self.df_gcp, col_mapping, is_training=True)
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        features_clean = features.dropna(subset=['CPUUsage', 'MemoryUsage'])
        features_clean = features_clean[
            (features_clean['CPUUsage'] > 0) & 
            (features_clean['CPUUsage'] <= 1) &
            (features_clean['MemoryUsage'] > 0) & 
            (features_clean['MemoryUsage'] <= 1)
        ]
        
        print(f"\n   ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(features_clean):,}ê±´")
        
        # Feature ì¸ì½”ë”©
        X = self._encode_features(features_clean, fit=True)
        y_cpu = features_clean['CPUUsage'].values
        y_memory = features_clean['MemoryUsage'].values
        
        # Train/Test ë¶„í• 
        X_train, X_test, y_cpu_train, y_cpu_test = train_test_split(
            X, y_cpu, test_size=0.2, random_state=42
        )
        _, _, y_mem_train, y_mem_test = train_test_split(
            X, y_memory, test_size=0.2, random_state=42
        )
        
        print(f"   ğŸ“Š Train: {len(X_train):,}ê±´, Test: {len(X_test):,}ê±´")
        
        # CPU ëª¨ë¸ í•™ìŠµ
        print(f"\n   ğŸ¤– CPU ëª¨ë¸ í•™ìŠµ ì¤‘...")
        self.cpu_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            n_jobs=-1
        )
        self.cpu_model.fit(X_train, y_cpu_train)
        
        # CPU ëª¨ë¸ í‰ê°€
        y_cpu_pred = self.cpu_model.predict(X_test)
        cpu_mae = mean_absolute_error(y_cpu_test, y_cpu_pred)
        cpu_r2 = r2_score(y_cpu_test, y_cpu_pred)
        
        print(f"      âœ… CPU ëª¨ë¸ MAE: {cpu_mae*100:.2f}%")
        print(f"      âœ… CPU ëª¨ë¸ RÂ²: {cpu_r2:.4f}")
        
        # Memory ëª¨ë¸ í•™ìŠµ
        print(f"\n   ğŸ¤– Memory ëª¨ë¸ í•™ìŠµ ì¤‘...")
        self.memory_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            n_jobs=-1
        )
        self.memory_model.fit(X_train, y_mem_train)
        
        # Memory ëª¨ë¸ í‰ê°€
        y_mem_pred = self.memory_model.predict(X_test)
        mem_mae = mean_absolute_error(y_mem_test, y_mem_pred)
        mem_r2 = r2_score(y_mem_test, y_mem_pred)
        
        print(f"      âœ… Memory ëª¨ë¸ MAE: {mem_mae*100:.2f}%")
        print(f"      âœ… Memory ëª¨ë¸ RÂ²: {mem_r2:.4f}")
        
        # Feature ì¤‘ìš”ë„
        print(f"\n   ğŸ“Š Feature ì¤‘ìš”ë„ (CPU):")
        importances = self.cpu_model.feature_importances_
        for i, col in enumerate(self.feature_cols):
            print(f"      â€¢ {col}: {importances[i]*100:.1f}%")
        
        # ëª¨ë¸ ì €ì¥
        self._save_models()
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        self.training_results = {
            'cpu_mae': cpu_mae,
            'cpu_r2': cpu_r2,
            'memory_mae': mem_mae,
            'memory_r2': mem_r2,
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'feature_importance': dict(zip(self.feature_cols, importances.tolist()))
        }
        
        return self
    
    
    def _save_models(self):
        """
        í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        """
        self.model_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        joblib.dump(self.cpu_model, self.model_output_dir / 'cpu_model.joblib')
        joblib.dump(self.memory_model, self.model_output_dir / 'memory_model.joblib')
        joblib.dump(self.label_encoders, self.model_output_dir / 'label_encoders.joblib')
        joblib.dump(self.scaler, self.model_output_dir / 'scaler.joblib')
        joblib.dump(self.feature_cols, self.model_output_dir / 'feature_cols.joblib')
        
        print(f"\n   ğŸ’¾ ëª¨ë¸ ì €ì¥: {self.model_output_dir}")
    
    
    def load_models(self):
        """
        ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
        
        Returns:
            self
        """
        self.print_step("ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ", f"{self.model_output_dir}")
        
        self.cpu_model = joblib.load(self.model_output_dir / 'cpu_model.joblib')
        self.memory_model = joblib.load(self.model_output_dir / 'memory_model.joblib')
        self.label_encoders = joblib.load(self.model_output_dir / 'label_encoders.joblib')
        self.scaler = joblib.load(self.model_output_dir / 'scaler.joblib')
        self.feature_cols = joblib.load(self.model_output_dir / 'feature_cols.joblib')
        
        self.print_success("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        return self
    
    
    def predict_aws(self):
        """
        AWS ë°ì´í„°ì— ëŒ€í•´ ì‚¬ìš©ë¥  ì˜ˆì¸¡
        
        Returns:
            self
        """
        self.print_step("AWS ë°ì´í„° ì˜ˆì¸¡")
        
        if self.cpu_model is None:
            self.print_error("ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ê±°ë‚˜ ë¡œë“œí•˜ì„¸ìš”.")
            return self
        
        # AWS ë°ì´í„° ë¡œë“œ
        if not self.aws_data_path.exists():
            self.print_error(f"AWS ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.aws_data_path}")
            return self
        
        self.df_aws = pd.read_csv(self.aws_data_path)
        print(f"   ğŸ“Š AWS ë°ì´í„°: {len(self.df_aws):,}ê±´")
        
        # AWS ì»¬ëŸ¼ ë§¤í•‘
        aws_col_mapping = {
            'service': 'ServiceName',
            'cost': 'BilledCost',
            'date': 'ChargePeriodStart',
            'cost_per_unit': None,
            'cpu': None,
            'memory': None
        }
        
        # ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
        for key, col in aws_col_mapping.items():
            if col and col not in self.df_aws.columns:
                aws_col_mapping[key] = None
        
        # Feature ì¶”ì¶œ
        features = self._extract_features(self.df_aws, aws_col_mapping, is_training=False)
        
        # Feature ì¸ì½”ë”©
        X = self._encode_features(features, fit=False)
        
        # ì˜ˆì¸¡
        print(f"\n   ğŸ”® ì˜ˆì¸¡ ì¤‘...")
        cpu_predictions = self.cpu_model.predict(X)
        memory_predictions = self.memory_model.predict(X)
        
        # ê²°ê³¼ ì €ì¥
        self.df_predictions = self.df_aws.copy()
        self.df_predictions['PredictedCPU'] = cpu_predictions
        self.df_predictions['PredictedMemory'] = memory_predictions
        self.df_predictions['ServiceCategory'] = features['ServiceCategory'].values
        self.df_predictions['ResourceType'] = features['ResourceType'].values
        
        # ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íŒì •
        self.df_predictions['IsOverProvisioned'] = (
            (self.df_predictions['PredictedCPU'] < self.cpu_threshold) |
            (self.df_predictions['PredictedMemory'] < self.memory_threshold)
        )
        
        # ë‚­ë¹„ìœ¨ ê³„ì‚°
        self.df_predictions['CPUWastePercent'] = (
            (1 - self.df_predictions['PredictedCPU']) * 100
        )
        self.df_predictions['MemoryWastePercent'] = (
            (1 - self.df_predictions['PredictedMemory']) * 100
        )
        
        # ì˜ˆìƒ ì ˆê°ì•¡
        if 'BilledCost' in self.df_predictions.columns:
            self.df_predictions['PotentialSavings'] = np.where(
                self.df_predictions['IsOverProvisioned'],
                self.df_predictions['BilledCost'] * 0.6,
                0
            )
        
        # ê²°ê³¼ í†µê³„
        self._print_prediction_summary()
        
        return self
    
    
    def _print_prediction_summary(self):
        """ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½"""
        print(f"\n{'='*100}")
        print("ğŸ“Š AWS ì‚¬ìš©ë¥  ì˜ˆì¸¡ ê²°ê³¼ (ML ê¸°ë°˜)")
        print(f"{'='*100}")
        
        total = len(self.df_predictions)
        over_prov = self.df_predictions['IsOverProvisioned'].sum()
        over_prov_rate = over_prov / total * 100
        
        print(f"\n   ğŸš¨ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€:")
        print(f"      â€¢ ì „ì²´: {total:,}ê±´")
        print(f"      â€¢ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {over_prov:,}ê±´ ({over_prov_rate:.1f}%)")
        print(f"      â€¢ ì •ìƒ: {total - over_prov:,}ê±´")
        
        # ì˜ˆì¸¡ê°’ ë¶„í¬
        print(f"\n   ğŸ“Š ì˜ˆì¸¡ ì‚¬ìš©ë¥  ë¶„í¬:")
        print(f"      â€¢ CPU í‰ê· : {self.df_predictions['PredictedCPU'].mean()*100:.1f}%")
        print(f"      â€¢ CPU ì¤‘ì•™ê°’: {self.df_predictions['PredictedCPU'].median()*100:.1f}%")
        print(f"      â€¢ Memory í‰ê· : {self.df_predictions['PredictedMemory'].mean()*100:.1f}%")
        print(f"      â€¢ Memory ì¤‘ì•™ê°’: {self.df_predictions['PredictedMemory'].median()*100:.1f}%")
        
        # ì„ê³„ê°’ ì´í•˜ ë¹„ìœ¨
        below_cpu = (self.df_predictions['PredictedCPU'] < self.cpu_threshold).sum()
        below_mem = (self.df_predictions['PredictedMemory'] < self.memory_threshold).sum()
        
        print(f"\n   ğŸ“‰ ì„ê³„ê°’({self.cpu_threshold*100:.0f}%) ì´í•˜:")
        print(f"      â€¢ CPU < {self.cpu_threshold*100:.0f}%: {below_cpu:,}ê±´ ({below_cpu/total*100:.1f}%)")
        print(f"      â€¢ Memory < {self.memory_threshold*100:.0f}%: {below_mem:,}ê±´ ({below_mem/total*100:.1f}%)")
        
        # ì¹´í…Œê³ ë¦¬ë³„
        if over_prov > 0:
            print(f"\n   ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹:")
            category_stats = self.df_predictions[self.df_predictions['IsOverProvisioned']].groupby(
                'ServiceCategory'
            ).size().sort_values(ascending=False)
            
            for cat, count in category_stats.head(5).items():
                pct = count / over_prov * 100
                print(f"      â€¢ {cat}: {count:,}ê±´ ({pct:.1f}%)")
        
        # ì˜ˆìƒ ì ˆê°ì•¡
        if 'PotentialSavings' in self.df_predictions.columns:
            total_savings = self.df_predictions['PotentialSavings'].sum()
            print(f"\n   ğŸ’° ì˜ˆìƒ ì ˆê°ì•¡:")
            print(f"      â€¢ ì´ ì ˆê° ê°€ëŠ¥: ${total_savings:,.2f}")
            print(f"      â€¢ ì—°ê°„ ì¶”ì •: ${total_savings * 12:,.2f}")
        
        print(f"\n{'='*100}")
    
    
    def save(self):
        """
        ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        
        Returns:
            self
        """
        if self.df_predictions is None:
            self.print_warning("ì €ì¥í•  ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return self
        
        self.print_step("ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥", f"{self.result_output_path}")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.ensure_dir(self.result_output_path.parent)
        
        # CSV ì €ì¥
        self.df_predictions.to_csv(self.result_output_path, index=False)
        
        self.print_success("ì €ì¥ ì™„ë£Œ")
        print(f"   ğŸ“‚ ê²½ë¡œ: {self.result_output_path}")
        print(f"   ğŸ“Š ë ˆì½”ë“œ: {len(self.df_predictions):,}ê±´")
        
        # ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ë§Œ ë³„ë„ ì €ì¥
        over_prov_path = self.result_output_path.parent / 'ml_overprovisioned.csv'
        df_over = self.df_predictions[self.df_predictions['IsOverProvisioned']]
        
        if len(df_over) > 0:
            df_over.to_csv(over_prov_path, index=False)
            print(f"   ğŸ“‚ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {over_prov_path}")
            print(f"   ğŸ“Š ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {len(df_over):,}ê±´")
        
        return self
    
    
    def run(self):
        """
        ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰: í•™ìŠµ â†’ ì˜ˆì¸¡ â†’ ì €ì¥
        
        Returns:
            self
        """
        return (self.load()
                .process()
                .predict_aws()
                .save())
    
    
    def get_results(self):
        """
        ê²°ê³¼ ë°˜í™˜
        
        Returns:
            tuple: (ì˜ˆì¸¡ ê²°ê³¼ DataFrame, í•™ìŠµ ê²°ê³¼ dict)
        """
        return (self.df_predictions, getattr(self, 'training_results', None))
    
    
    def get_overprovisioned(self):
        """
        ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ ë°ì´í„°ë§Œ ë°˜í™˜
        
        Returns:
            DataFrame: ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ ë°ì´í„°
        """
        if self.df_predictions is None:
            return None
        
        return self.df_predictions[self.df_predictions['IsOverProvisioned']].copy()


# ==================== ë©”ì¸ ì‹¤í–‰ ====================
if __name__ == "__main__":
    
    print("\nğŸš€ ML ê¸°ë°˜ ì‚¬ìš©ë¥  ì˜ˆì¸¡ ì‹œì‘")
    print("="*100)
    
    predictor = MLUsagePredictor('config/focus_config.yaml')
    predictor.run()
    
    # ê²°ê³¼ ì¡°íšŒ
    df_predictions, training_results = predictor.get_results()
    
    print(f"\nâœ… ì™„ë£Œ!")
    print(f"   ì „ì²´ ì˜ˆì¸¡: {len(df_predictions):,}ê±´")
    
    df_over = predictor.get_overprovisioned()
    if df_over is not None:
        print(f"   ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {len(df_over):,}ê±´")
    
    if training_results:
        print(f"\nğŸ“Š í•™ìŠµ ê²°ê³¼:")
        print(f"   CPU MAE: {training_results['cpu_mae']*100:.2f}%")
        print(f"   CPU RÂ²: {training_results['cpu_r2']:.4f}")
        print(f"   Memory MAE: {training_results['memory_mae']*100:.2f}%")
        print(f"   Memory RÂ²: {training_results['memory_r2']:.4f}")