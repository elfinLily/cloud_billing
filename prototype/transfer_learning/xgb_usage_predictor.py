# -*- coding: utf-8 -*-
"""
XGBoost ê¸°ë°˜ ì‚¬ìš©ë¥  ì˜ˆì¸¡ ëª¨ë¸

============================================================
ë°ì´í„° íë¦„:
============================================================
1. resource_grouped.csv ë¡œë“œ (TimeNormalizer â†’ ResourceGrouper ê²°ê³¼)
2. ProviderNameìœ¼ë¡œ GCP/AWS ë¶„ë¦¬
3. GCP ë°ì´í„°ë¡œ í•™ìŠµ (AvgCPUUsage, AvgMemoryUsage ìˆìŒ)
4. AWS ë°ì´í„°ì— ì ìš©í•˜ì—¬ CPU/Memory ì‚¬ìš©ë¥  ì˜ˆì¸¡
5. ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ (ì‚¬ìš©ë¥  < 30%)

============================================================
Author: Lily
Date: 2025-01
Purpose: ì„ì‚¬ ë…¼ë¬¸ - LLM ê¸°ë°˜ í´ë¼ìš°ë“œ FinOps ìë™í™” ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ
============================================================
"""

import pandas as pd
import numpy as np
import yaml
import json
import joblib
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================
# XGBoost ë° sklearn ì„í¬íŠ¸
# ============================================================
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import sys

# ============================================================
# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
# ============================================================
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'data_processing'))

from pipeline_base import PipelineBase


class XGBUsagePredictor(PipelineBase):
    """
    XGBoost ê¸°ë°˜ ì‚¬ìš©ë¥  ì˜ˆì¸¡ í´ë˜ìŠ¤
    
    ============================================================
    ì£¼ìš” ê¸°ëŠ¥:
    ============================================================
    1. resource_grouped.csvì—ì„œ GCP/AWS ë¶„ë¦¬
    2. GCP ë°ì´í„°ë¡œ XGBoost ëª¨ë¸ í•™ìŠµ
    3. RandomizedSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    4. AWS ë°ì´í„°ì— ì ìš©í•˜ì—¬ CPU/Memory ì‚¬ìš©ë¥  ì˜ˆì¸¡
    5. ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€ (ì‚¬ìš©ë¥  < 30%)
    6. RandomForest ê²°ê³¼ì™€ ë¹„êµ
    
    ============================================================
    RandomForest ëŒ€ë¹„ XGBoost ì¥ì :
    ============================================================
    - Gradient Boostingìœ¼ë¡œ ìˆœì°¨ì  ì˜¤ì°¨ ë³´ì •
    - ì •í˜• ë°ì´í„°ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ë” ë†’ì€ ì„±ëŠ¥
    - ì •ê·œí™”(L1/L2) ë‚´ì¥ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
    """
    
    def __init__(self, config_path='config/focus_config.yaml'):
        """
        ì´ˆê¸°í™” - configì—ì„œ ê²½ë¡œì™€ ì„ê³„ê°’ ë¡œë“œ
        
        Args:
            config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        super().__init__(config_path)
        
        # --------------------------------------------------------
        # ê²½ë¡œ ì„¤ì • (config ê¸°ë°˜, í•˜ë“œì½”ë”© ê¸ˆì§€)
        # --------------------------------------------------------
        data_config = self.config['data']
        
        # ì…ë ¥: resource_grouped.csv (ResourceGrouper ê²°ê³¼)
        self.input_path = Path(data_config['resource_grouped_output'])
        
        # ì¶œë ¥ ê²½ë¡œ
        self.model_output_dir = Path('results/transfer_learning/models/xgboost')
        self.result_output_path = Path('results/transfer_learning/xgb_predictions.csv')
        
        # --------------------------------------------------------
        # ì„ê³„ê°’ (config ê¸°ë°˜)
        # --------------------------------------------------------
        thresholds = self.config['thresholds']['over_provisioning']
        self.cpu_threshold = thresholds['cpu_threshold']      # 0.30
        self.memory_threshold = thresholds['memory_threshold']  # 0.30
        
        # --------------------------------------------------------
        # ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸°
        # --------------------------------------------------------
        self.cpu_model = None
        self.memory_model = None
        self.label_encoders = {}
        self.scaler = StandardScaler()
        
        # --------------------------------------------------------
        # ë°ì´í„° ì €ì¥ìš©
        # --------------------------------------------------------
        self.df_all = None       # ì „ì²´ ë°ì´í„°
        self.df_gcp = None       # GCP ë°ì´í„° (í•™ìŠµìš©)
        self.df_aws = None       # AWS ë°ì´í„° (ì˜ˆì¸¡ ëŒ€ìƒ)
        self.df_predictions = None
        self.training_results = None
        
        # --------------------------------------------------------
        # Feature ì»¬ëŸ¼ ì •ì˜ (ì´ì „ ëŒ€í™”ì—ì„œ í™•ì •ëœ 5ê°œ)
        # --------------------------------------------------------
        self.feature_cols = []
        self.categorical_cols = ['ServiceName', 'ResourceType']
        self.numerical_cols = ['TotalHourlyCost', 'HourOfDay', 'DayOfWeek']
        
        # --------------------------------------------------------
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì„¤ì •
        # --------------------------------------------------------
        self.tune_hyperparams = True
        self.sample_size = 5_000_000  # íŠœë‹ìš© ìƒ˜í”Œ í¬ê¸°
        self.n_iter = 15              # RandomizedSearchCV ë°˜ë³µ íšŸìˆ˜
        self.cv_folds = 3             # Cross-validation í´ë“œ ìˆ˜
    
    
    def load(self):
        """
        resource_grouped.csv ë¡œë“œ ë° GCP/AWS ë¶„ë¦¬
        
        --------------------------------------------------------
        ê¸°ëŠ¥:
        - resource_grouped.csv íŒŒì¼ ë¡œë“œ
        - ProviderName ê¸°ì¤€ìœ¼ë¡œ GCP/AWS ë¶„ë¦¬
        - GCP: í•™ìŠµìš© (AvgCPUUsage, AvgMemoryUsage ìˆìŒ)
        - AWS: ì˜ˆì¸¡ ëŒ€ìƒ (ì‚¬ìš©ë¥  ì—†ìŒ)
        --------------------------------------------------------
        
        Returns:
            self: ë©”ì„œë“œ ì²´ì´ë‹ìš©
        """
        self.print_step("ë°ì´í„° ë¡œë”©", f"{self.input_path}")
        
        if not self.input_path.exists():
            self.print_error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.input_path}")
            self.print_warning("ë¨¼ì € ResourceGrouperë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return self
        
        # CSV ë¡œë“œ
        self.df_all = pd.read_csv(self.input_path)
        
        self.print_success("ë¡œë“œ ì™„ë£Œ")
        print(f"   ğŸ“Š ì „ì²´ ë ˆì½”ë“œ: {len(self.df_all):,}ê±´")
        print(f"   ğŸ“‹ ì»¬ëŸ¼: {list(self.df_all.columns)}")
        
        # --------------------------------------------------------
        # ProviderNameìœ¼ë¡œ GCP/AWS ë¶„ë¦¬
        # --------------------------------------------------------
        if 'ProviderName' not in self.df_all.columns:
            self.print_error("ProviderName ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return self
        
        # GCP ë°ì´í„° í•„í„°ë§
        gcp_mask = self.df_all['ProviderName'].str.upper().str.contains('GCP|GOOGLE', na=False)
        self.df_gcp = self.df_all[gcp_mask].copy()
        
        # AWS ë°ì´í„° í•„í„°ë§
        aws_mask = self.df_all['ProviderName'].str.upper().str.contains('AWS|AMAZON', na=False)
        self.df_aws = self.df_all[aws_mask].copy()
        
        print(f"\n   â˜ï¸  Providerë³„ ë¶„ë¦¬:")
        print(f"      â€¢ GCP (í•™ìŠµìš©): {len(self.df_gcp):,}ê±´")
        print(f"      â€¢ AWS (ì˜ˆì¸¡ ëŒ€ìƒ): {len(self.df_aws):,}ê±´")
        
        # CPU/Memory ì»¬ëŸ¼ í™•ì¸
        cpu_col = self._find_column(['AvgCPUUsage', 'SimulatedCPUUsage', 'CPUUsage'])
        mem_col = self._find_column(['AvgMemoryUsage', 'SimulatedMemoryUsage', 'MemoryUsage'])
        
        print(f"\n   ğŸ“‹ ì‚¬ìš©ë¥  ì»¬ëŸ¼:")
        print(f"      â€¢ CPU: {cpu_col}")
        print(f"      â€¢ Memory: {mem_col}")
        
        return self
    
    
    def _find_column(self, candidates):
        """
        í›„ë³´ ì»¬ëŸ¼ ì¤‘ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸°
        
        Args:
            candidates (list): í›„ë³´ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
        
        Returns:
            str or None: ì°¾ì€ ì»¬ëŸ¼ëª…
        """
        if self.df_all is None:
            return None
        
        for col in candidates:
            if col in self.df_all.columns:
                return col
        return None
    
    
    def _extract_features(self, df, is_training=True):
        """
        Feature ì¶”ì¶œ ë° ì „ì²˜ë¦¬
        
        --------------------------------------------------------
        Feature 5ê°œ:
        1. ServiceName (ë²”ì£¼í˜•)
        2. ResourceType (ë²”ì£¼í˜•) - ì—†ìœ¼ë©´ ServiceNameì—ì„œ ì¶”ì¶œ
        3. TotalHourlyCost (ìˆ˜ì¹˜í˜•)
        4. HourOfDay (ìˆ˜ì¹˜í˜•)
        5. DayOfWeek (ìˆ˜ì¹˜í˜•)
        --------------------------------------------------------
        
        Args:
            df (DataFrame): ì›ë³¸ ë°ì´í„°
            is_training (bool): í•™ìŠµ ë°ì´í„° ì—¬ë¶€
        
        Returns:
            DataFrame: Feature DataFrame
        """
        features = pd.DataFrame()
        
        # 1. ServiceName
        if 'ServiceName' in df.columns:
            features['ServiceName'] = df['ServiceName'].fillna('Unknown')
        else:
            features['ServiceName'] = 'Unknown'
        
        # 2. ResourceType (ì—†ìœ¼ë©´ ServiceNameì—ì„œ ì¶”ì¶œ)
        if 'ResourceType' in df.columns:
            features['ResourceType'] = df['ResourceType'].fillna('Other')
        else:
            features['ResourceType'] = features['ServiceName'].apply(self._extract_resource_type)
        
        # 3. TotalHourlyCost
        if 'TotalHourlyCost' in df.columns:
            features['TotalHourlyCost'] = pd.to_numeric(
                df['TotalHourlyCost'], errors='coerce'
            ).fillna(0)
        elif 'BilledCost' in df.columns:
            features['TotalHourlyCost'] = pd.to_numeric(
                df['BilledCost'], errors='coerce'
            ).fillna(0)
        else:
            features['TotalHourlyCost'] = 0
        
        # 4 & 5. HourOfDay, DayOfWeek
        time_col = self._find_time_column(df)
        if time_col:
            try:
                dt = pd.to_datetime(df[time_col], errors='coerce')
                features['HourOfDay'] = dt.dt.hour.fillna(12).astype(int)
                features['DayOfWeek'] = dt.dt.dayofweek.fillna(3).astype(int)
            except:
                features['HourOfDay'] = 12
                features['DayOfWeek'] = 3
        else:
            features['HourOfDay'] = 12
            features['DayOfWeek'] = 3
        
        # Target ì»¬ëŸ¼ (í•™ìŠµ ì‹œì—ë§Œ)
        if is_training:
            # CPU ì‚¬ìš©ë¥ 
            cpu_col = self._find_column(['AvgCPUUsage', 'SimulatedCPUUsage', 'CPUUsage'])
            if cpu_col and cpu_col in df.columns:
                features['CPUUsage'] = pd.to_numeric(df[cpu_col], errors='coerce')
            
            # Memory ì‚¬ìš©ë¥ 
            mem_col = self._find_column(['AvgMemoryUsage', 'SimulatedMemoryUsage', 'MemoryUsage'])
            if mem_col and mem_col in df.columns:
                features['MemoryUsage'] = pd.to_numeric(df[mem_col], errors='coerce')
        
        return features
    
    
    def _find_time_column(self, df):
        """
        ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
        
        Args:
            df (DataFrame): ë°ì´í„°í”„ë ˆì„
        
        Returns:
            str or None: ì‹œê°„ ì»¬ëŸ¼ëª…
        """
        candidates = ['HourlyTimestamp', 'ChargePeriodStart', 'Date', 'Timestamp', 'Hour']
        for col in candidates:
            if col in df.columns:
                return col
        return None
    
    
    def _extract_resource_type(self, service_name):
        """
        ì„œë¹„ìŠ¤ëª…ì—ì„œ ë¦¬ì†ŒìŠ¤ íƒ€ì… ì¶”ì¶œ
        
        Args:
            service_name: ì„œë¹„ìŠ¤ëª…
        
        Returns:
            str: ë¦¬ì†ŒìŠ¤ íƒ€ì… (VM, Container, Function ë“±)
        """
        if pd.isna(service_name):
            return 'Other'
        
        service_lower = str(service_name).lower()
        
        if any(kw in service_lower for kw in ['vm', 'instance', 'engine', 'compute', 'ec2']):
            return 'VM'
        elif any(kw in service_lower for kw in ['container', 'kubernetes', 'ecs', 'eks', 'gke']):
            return 'Container'
        elif any(kw in service_lower for kw in ['function', 'lambda', 'cloud functions']):
            return 'Function'
        elif any(kw in service_lower for kw in ['storage', 'bucket', 's3', 'gcs']):
            return 'ObjectStorage'
        elif any(kw in service_lower for kw in ['disk', 'volume', 'ebs']):
            return 'BlockStorage'
        elif any(kw in service_lower for kw in ['sql', 'database', 'rds', 'spanner', 'dynamodb']):
            return 'Database'
        elif any(kw in service_lower for kw in ['network', 'vpc', 'load balancer', 'cdn']):
            return 'Network'
        else:
            return 'Other'
    
    
    def _encode_features(self, features, fit=True):
        """
        ì¹´í…Œê³ ë¦¬ Feature ì¸ì½”ë”© + ìˆ˜ì¹˜í˜• ì •ê·œí™”
        
        --------------------------------------------------------
        ì²˜ë¦¬:
        - LabelEncoder: ë²”ì£¼í˜• â†’ ì •ìˆ˜
        - StandardScaler: ìˆ˜ì¹˜í˜• ì •ê·œí™”
        - í•™ìŠµ ì‹œ ì—†ë˜ ì¹´í…Œê³ ë¦¬ â†’ ì²« ë²ˆì§¸ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´
        --------------------------------------------------------
        
        Args:
            features (DataFrame): Feature DataFrame
            fit (bool): ì¸ì½”ë” í•™ìŠµ ì—¬ë¶€
        
        Returns:
            numpy.ndarray: ì¸ì½”ë”©ëœ Feature ë°°ì—´
        """
        df_encoded = features.copy()
        
        # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ì¸ì½”ë”©
        for col in self.categorical_cols:
            if col in df_encoded.columns:
                if fit:
                    self.label_encoders[col] = LabelEncoder()
                    df_encoded[col] = self.label_encoders[col].fit_transform(
                        df_encoded[col].astype(str)
                    )
                else:
                    # í•™ìŠµ ì‹œ ì—†ë˜ ì¹´í…Œê³ ë¦¬ â†’ ì²« ë²ˆì§¸ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´
                    le = self.label_encoders.get(col)
                    if le:
                        df_encoded[col] = df_encoded[col].astype(str).apply(
                            lambda x: le.transform([x])[0] if x in le.classes_ 
                            else le.transform([le.classes_[0]])[0]
                        )
        
        # Feature ì»¬ëŸ¼ ì„ íƒ
        feature_cols = self.categorical_cols + self.numerical_cols
        feature_cols = [col for col in feature_cols if col in df_encoded.columns]
        
        X = df_encoded[feature_cols].values.astype(np.float32)
        
        # ìˆ˜ì¹˜í˜• ì •ê·œí™”
        if fit:
            X = self.scaler.fit_transform(X)
        else:
            X = self.scaler.transform(X)
        
        self.feature_cols = feature_cols
        
        return X
    
    
    def _get_xgb_param_space(self):
        """
        XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„ ì •ì˜
        
        --------------------------------------------------------
        íƒìƒ‰ íŒŒë¼ë¯¸í„°:
        - n_estimators: íŠ¸ë¦¬ ê°œìˆ˜ (50~300)
        - max_depth: íŠ¸ë¦¬ ê¹Šì´ (3~15)
        - learning_rate: í•™ìŠµë¥  (0.01~0.3)
        - subsample: í–‰ ìƒ˜í”Œë§ ë¹„ìœ¨ (0.6~1.0)
        - colsample_bytree: ì»¬ëŸ¼ ìƒ˜í”Œë§ ë¹„ìœ¨ (0.6~1.0)
        - reg_alpha: L1 ì •ê·œí™” (0~1)
        - reg_lambda: L2 ì •ê·œí™” (0~1)
        --------------------------------------------------------
        
        Returns:
            dict: íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„
        """
        param_space = {
            'n_estimators': [50, 100, 150, 200, 300],
            'max_depth': [3, 5, 7, 10, 15],
            'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
            'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
            'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
            'reg_alpha': [0, 0.1, 0.5, 1.0],
            'reg_lambda': [0, 0.1, 0.5, 1.0],
            'min_child_weight': [1, 3, 5, 7]
        }
        return param_space
    
    
    def _tune_model(self, X_train, y_train, target_name='CPU'):
        """
        RandomizedSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
        
        --------------------------------------------------------
        í”„ë¡œì„¸ìŠ¤:
        1. ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§ (500ë§Œ ê±´)
        2. RandomizedSearchCV ì‹¤í–‰
        3. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ë°ì´í„° ì¬í•™ìŠµ
        --------------------------------------------------------
        
        Args:
            X_train (ndarray): í•™ìŠµ Feature
            y_train (ndarray): í•™ìŠµ Target
            target_name (str): íƒ€ê²Ÿ ì´ë¦„ (CPU/Memory)
        
        Returns:
            tuple: (ìµœì  ëª¨ë¸, ìµœì  íŒŒë¼ë¯¸í„°, ìµœì  ì ìˆ˜)
        """
        print(f"\n   ğŸ”§ {target_name} ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹...")
        
        # ìƒ˜í”Œë§ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬)
        if len(X_train) > self.sample_size:
            indices = np.random.choice(len(X_train), self.sample_size, replace=False)
            X_sample = X_train[indices]
            y_sample = y_train[indices]
            print(f"      ğŸ“Š ìƒ˜í”Œë§: {len(X_train):,} â†’ {len(X_sample):,}ê±´")
        else:
            X_sample = X_train
            y_sample = y_train
        
        # ê¸°ë³¸ ëª¨ë¸
        base_model = XGBRegressor(
            objective='reg:squarederror',
            random_state=42,
            n_jobs=-1,
            verbosity=0
        )
        
        # RandomizedSearchCV
        param_space = self._get_xgb_param_space()
        
        search = RandomizedSearchCV(
            estimator=base_model,
            param_distributions=param_space,
            n_iter=self.n_iter,
            cv=self.cv_folds,
            scoring='neg_mean_absolute_error',
            random_state=42,
            n_jobs=-1,
            verbose=1
        )
        
        print(f"      ğŸ”„ RandomizedSearchCV ì‹¤í–‰ (n_iter={self.n_iter}, cv={self.cv_folds})")
        search.fit(X_sample, y_sample)
        
        best_params = search.best_params_
        best_score = -search.best_score_  # MAE (ì–‘ìˆ˜ë¡œ ë³€í™˜)
        
        print(f"\n      âœ… {target_name} ìµœì  íŒŒë¼ë¯¸í„°:")
        for key, val in best_params.items():
            print(f"         â€¢ {key}: {val}")
        print(f"      ğŸ“Š CV MAE: {best_score*100:.2f}%")
        
        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì  ëª¨ë¸ ì¬í•™ìŠµ
        print(f"\n      ğŸ”„ ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ì¤‘...")
        best_model = XGBRegressor(
            objective='reg:squarederror',
            random_state=42,
            n_jobs=-1,
            verbosity=0,
            **best_params
        )
        best_model.fit(X_train, y_train)
        
        return best_model, best_params, best_score
    
    
    def process(self):
        """
        XGBoost ëª¨ë¸ í•™ìŠµ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤)
        
        --------------------------------------------------------
        í”„ë¡œì„¸ìŠ¤:
        1. GCP ë°ì´í„°ì—ì„œ Feature ì¶”ì¶œ
        2. ê²°ì¸¡ì¹˜ ì œê±° ë° ìœ íš¨ ë°ì´í„° í•„í„°ë§
        3. Train/Test ë¶„í•  (80:20)
        4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì„ íƒì )
        5. CPU/Memory ëª¨ë¸ í•™ìŠµ
        6. í‰ê°€ ë° ê²°ê³¼ ì €ì¥
        --------------------------------------------------------
        
        Returns:
            self: ë©”ì„œë“œ ì²´ì´ë‹ìš©
        """
        self.print_step("XGBoost ëª¨ë¸ í•™ìŠµ (GCP ë°ì´í„°)")
        
        if self.df_gcp is None or len(self.df_gcp) == 0:
            self.print_error("GCP í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. load()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return self
        
        # Feature ì¶”ì¶œ
        print(f"\n   ğŸ“Š Feature ì¶”ì¶œ ì¤‘...")
        features = self._extract_features(self.df_gcp, is_training=True)
        
        print(f"      â€¢ ì¶”ì¶œëœ Feature: {list(features.columns)}")
        
        # CPU/Memory ì»¬ëŸ¼ í™•ì¸
        if 'CPUUsage' not in features.columns or 'MemoryUsage' not in features.columns:
            self.print_error("CPUUsage ë˜ëŠ” MemoryUsage ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"      â€¢ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(features.columns)}")
            return self
        
        # ê²°ì¸¡ì¹˜ ì œê±° ë° ìœ íš¨ ë²”ìœ„ í•„í„°ë§ (0 < ì‚¬ìš©ë¥  <= 1)
        features_clean = features.dropna(subset=['CPUUsage', 'MemoryUsage'])
        features_clean = features_clean[
            (features_clean['CPUUsage'] > 0) & 
            (features_clean['CPUUsage'] <= 1) &
            (features_clean['MemoryUsage'] > 0) & 
            (features_clean['MemoryUsage'] <= 1)
        ]
        
        print(f"\n   ğŸ“Š í•™ìŠµ ë°ì´í„°:")
        print(f"      â€¢ ì›ë³¸: {len(features):,}ê±´")
        print(f"      â€¢ ìœ íš¨: {len(features_clean):,}ê±´")
        
        if len(features_clean) == 0:
            self.print_error("ìœ íš¨í•œ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return self
        
        # Feature ì¸ì½”ë”©
        X = self._encode_features(features_clean, fit=True)
        y_cpu = features_clean['CPUUsage'].values
        y_memory = features_clean['MemoryUsage'].values
        
        print(f"   ğŸ“‹ Feature ì»¬ëŸ¼: {self.feature_cols}")
        print(f"   ğŸ“Š X shape: {X.shape}")
        
        # Train/Test ë¶„í•  (80:20)
        X_train, X_test, y_cpu_train, y_cpu_test = train_test_split(
            X, y_cpu, test_size=0.2, random_state=42
        )
        _, _, y_mem_train, y_mem_test = train_test_split(
            X, y_memory, test_size=0.2, random_state=42
        )
        
        print(f"\n   ğŸ“Š Train/Test ë¶„í• :")
        print(f"      â€¢ Train: {len(X_train):,}ê±´")
        print(f"      â€¢ Test: {len(X_test):,}ê±´")
        
        # ============================================================
        # CPU ëª¨ë¸ í•™ìŠµ
        # ============================================================
        if self.tune_hyperparams:
            self.cpu_model, cpu_best_params, _ = self._tune_model(
                X_train, y_cpu_train, target_name='CPU'
            )
        else:
            print(f"\n   ğŸ¤– CPU ëª¨ë¸ í•™ìŠµ ì¤‘ (ê¸°ë³¸ íŒŒë¼ë¯¸í„°)...")
            self.cpu_model = XGBRegressor(
                n_estimators=100,
                max_depth=10,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1,
                verbosity=0
            )
            self.cpu_model.fit(X_train, y_cpu_train)
            cpu_best_params = {}
        
        # CPU ëª¨ë¸ í‰ê°€
        y_cpu_pred = self.cpu_model.predict(X_test)
        cpu_mae = mean_absolute_error(y_cpu_test, y_cpu_pred)
        cpu_rmse = np.sqrt(mean_squared_error(y_cpu_test, y_cpu_pred))
        cpu_r2 = r2_score(y_cpu_test, y_cpu_pred)
        
        print(f"\n   ğŸ“Š CPU ëª¨ë¸ ì„±ëŠ¥:")
        print(f"      â€¢ MAE: {cpu_mae*100:.2f}%")
        print(f"      â€¢ RMSE: {cpu_rmse*100:.2f}%")
        print(f"      â€¢ RÂ²: {cpu_r2:.4f}")
        
        # ============================================================
        # Memory ëª¨ë¸ í•™ìŠµ
        # ============================================================
        if self.tune_hyperparams:
            self.memory_model, mem_best_params, _ = self._tune_model(
                X_train, y_mem_train, target_name='Memory'
            )
        else:
            print(f"\n   ğŸ¤– Memory ëª¨ë¸ í•™ìŠµ ì¤‘ (ê¸°ë³¸ íŒŒë¼ë¯¸í„°)...")
            self.memory_model = XGBRegressor(
                n_estimators=100,
                max_depth=10,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1,
                verbosity=0
            )
            self.memory_model.fit(X_train, y_mem_train)
            mem_best_params = {}
        
        # Memory ëª¨ë¸ í‰ê°€
        y_mem_pred = self.memory_model.predict(X_test)
        mem_mae = mean_absolute_error(y_mem_test, y_mem_pred)
        mem_rmse = np.sqrt(mean_squared_error(y_mem_test, y_mem_pred))
        mem_r2 = r2_score(y_mem_test, y_mem_pred)
        
        print(f"\n   ğŸ“Š Memory ëª¨ë¸ ì„±ëŠ¥:")
        print(f"      â€¢ MAE: {mem_mae*100:.2f}%")
        print(f"      â€¢ RMSE: {mem_rmse*100:.2f}%")
        print(f"      â€¢ RÂ²: {mem_r2:.4f}")
        
        # ============================================================
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        # ============================================================
        self.training_results = {
            'model_type': 'XGBoost',
            'timestamp': datetime.now().isoformat(),
            'train_size': int(len(X_train)),
            'test_size': int(len(X_test)),
            'feature_cols': self.feature_cols,
            'cpu_mae': float(cpu_mae),
            'cpu_rmse': float(cpu_rmse),
            'cpu_r2': float(cpu_r2),
            'cpu_best_params': cpu_best_params if self.tune_hyperparams else {},
            'memory_mae': float(mem_mae),
            'memory_rmse': float(mem_rmse),
            'memory_r2': float(mem_r2),
            'memory_best_params': mem_best_params if self.tune_hyperparams else {},
            'tuning_config': {
                'tune_hyperparams': self.tune_hyperparams,
                'sample_size': self.sample_size,
                'n_iter': self.n_iter,
                'cv_folds': self.cv_folds
            }
        }
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self._print_training_summary()
        
        # ëª¨ë¸ ì €ì¥
        self._save_models()
        
        return self
    
    
    def _print_training_summary(self):
        """
        í•™ìŠµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (RandomForest ë¹„êµ í¬í•¨)
        """
        print(f"\n{'='*100}")
        print("ğŸ“Š XGBoost í•™ìŠµ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*100}")
        
        r = self.training_results
        
        print(f"\n   ğŸ”¢ ë°ì´í„°:")
        print(f"      â€¢ Train: {r['train_size']:,}ê±´")
        print(f"      â€¢ Test: {r['test_size']:,}ê±´")
        
        print(f"\n   ğŸ“Š CPU ëª¨ë¸:")
        print(f"      â€¢ MAE: {r['cpu_mae']*100:.2f}%")
        print(f"      â€¢ RÂ²: {r['cpu_r2']:.4f}")
        
        print(f"\n   ğŸ“Š Memory ëª¨ë¸:")
        print(f"      â€¢ MAE: {r['memory_mae']*100:.2f}%")
        print(f"      â€¢ RÂ²: {r['memory_r2']:.4f}")
        
        # RandomForest ê²°ê³¼ì™€ ë¹„êµ (ì´ì „ ëŒ€í™”ì—ì„œ í™•ì¸ëœ ê²°ê³¼)
        rf_cpu_mae = 0.2369
        rf_cpu_r2 = 0.0895
        rf_mem_mae = 0.2371
        rf_mem_r2 = 0.0947
        
        print(f"\n   ğŸ“ˆ RandomForest ëŒ€ë¹„ ë¹„êµ:")
        
        # MAE ê°œì„  (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ â†’ ì–‘ìˆ˜ë©´ ê°œì„ )
        cpu_mae_diff = (rf_cpu_mae - r['cpu_mae']) * 100
        mem_mae_diff = (rf_mem_mae - r['memory_mae']) * 100
        
        # RÂ² ê°œì„  (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ â†’ ì–‘ìˆ˜ë©´ ê°œì„ )
        cpu_r2_diff = r['cpu_r2'] - rf_cpu_r2
        mem_r2_diff = r['memory_r2'] - rf_mem_r2
        
        cpu_mae_sign = "+" if cpu_mae_diff > 0 else ""
        cpu_r2_sign = "+" if cpu_r2_diff > 0 else ""
        mem_mae_sign = "+" if mem_mae_diff > 0 else ""
        mem_r2_sign = "+" if mem_r2_diff > 0 else ""
        
        print(f"      â€¢ CPU MAE: {cpu_mae_sign}{cpu_mae_diff:.2f}%p (RF: {rf_cpu_mae*100:.2f}% â†’ XGB: {r['cpu_mae']*100:.2f}%)")
        print(f"      â€¢ CPU RÂ²: {cpu_r2_sign}{cpu_r2_diff:.4f} (RF: {rf_cpu_r2:.4f} â†’ XGB: {r['cpu_r2']:.4f})")
        print(f"      â€¢ Memory MAE: {mem_mae_sign}{mem_mae_diff:.2f}%p (RF: {rf_mem_mae*100:.2f}% â†’ XGB: {r['memory_mae']*100:.2f}%)")
        print(f"      â€¢ Memory RÂ²: {mem_r2_sign}{mem_r2_diff:.4f} (RF: {rf_mem_r2:.4f} â†’ XGB: {r['memory_r2']:.4f})")
        
        print(f"\n{'='*100}")
    
    
    def _save_models(self):
        """
        í•™ìŠµëœ ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì €ì¥
        """
        self.model_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        joblib.dump(self.cpu_model, self.model_output_dir / 'cpu_model.joblib')
        joblib.dump(self.memory_model, self.model_output_dir / 'memory_model.joblib')
        joblib.dump(self.label_encoders, self.model_output_dir / 'label_encoders.joblib')
        joblib.dump(self.scaler, self.model_output_dir / 'scaler.joblib')
        joblib.dump(self.feature_cols, self.model_output_dir / 'feature_cols.joblib')
        
        # í•™ìŠµ ê²°ê³¼ JSON ì €ì¥
        results_path = self.model_output_dir / 'training_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_results, f, indent=2, ensure_ascii=False)
        
        print(f"\n   ğŸ’¾ ëª¨ë¸ ì €ì¥: {self.model_output_dir}")
        print(f"   ğŸ’¾ í•™ìŠµ ê²°ê³¼: {results_path}")
    
    
    def load_models(self):
        """
        ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
        
        Returns:
            self: ë©”ì„œë“œ ì²´ì´ë‹ìš©
        """
        self.print_step("ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ", f"{self.model_output_dir}")
        
        self.cpu_model = joblib.load(self.model_output_dir / 'cpu_model.joblib')
        self.memory_model = joblib.load(self.model_output_dir / 'memory_model.joblib')
        self.label_encoders = joblib.load(self.model_output_dir / 'label_encoders.joblib')
        self.scaler = joblib.load(self.model_output_dir / 'scaler.joblib')
        self.feature_cols = joblib.load(self.model_output_dir / 'feature_cols.joblib')
        
        self.print_success("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        return self
    
    
    def predict_aws(self):
        """
        AWS ë°ì´í„°ì— ëŒ€í•´ ì‚¬ìš©ë¥  ì˜ˆì¸¡
        
        --------------------------------------------------------
        í”„ë¡œì„¸ìŠ¤:
        1. AWS ë°ì´í„°ì—ì„œ Feature ì¶”ì¶œ
        2. Feature ì¸ì½”ë”© (í•™ìŠµëœ ì¸ì½”ë” ì‚¬ìš©)
        3. CPU/Memory ì‚¬ìš©ë¥  ì˜ˆì¸¡
        4. ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íŒì • (< 30%)
        --------------------------------------------------------
        
        Returns:
            self: ë©”ì„œë“œ ì²´ì´ë‹ìš©
        """
        self.print_step("AWS ë°ì´í„° ì˜ˆì¸¡")
        
        if self.cpu_model is None:
            self.print_error("ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ê±°ë‚˜ ë¡œë“œí•˜ì„¸ìš”.")
            return self
        
        if self.df_aws is None or len(self.df_aws) == 0:
            self.print_warning("AWS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return self
        
        print(f"   ğŸ“Š AWS ë°ì´í„°: {len(self.df_aws):,}ê±´")
        
        # Feature ì¶”ì¶œ
        features = self._extract_features(self.df_aws, is_training=False)
        
        # Feature ì¸ì½”ë”©
        X = self._encode_features(features, fit=False)
        
        # ì˜ˆì¸¡
        print(f"\n   ğŸ”® ì˜ˆì¸¡ ì¤‘...")
        cpu_predictions = self.cpu_model.predict(X)
        memory_predictions = self.memory_model.predict(X)
        
        # ì˜ˆì¸¡ê°’ í´ë¦¬í•‘ (0~1 ë²”ìœ„)
        cpu_predictions = np.clip(cpu_predictions, 0, 1)
        memory_predictions = np.clip(memory_predictions, 0, 1)
        
        # ê²°ê³¼ ì €ì¥
        self.df_predictions = self.df_aws.copy()
        self.df_predictions['PredictedCPU'] = cpu_predictions
        self.df_predictions['PredictedMemory'] = memory_predictions
        
        # ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íŒì • (CPU ë˜ëŠ” Memory < ì„ê³„ê°’)
        self.df_predictions['IsOverProvisioned'] = (
            (self.df_predictions['PredictedCPU'] < self.cpu_threshold) |
            (self.df_predictions['PredictedMemory'] < self.memory_threshold)
        )
        
        # ë‚­ë¹„ìœ¨ ê³„ì‚°
        self.df_predictions['CPUWastePercent'] = (
            (1 - self.df_predictions['PredictedCPU']) * 100
        )
        self.df_predictions['MemoryWastePercent'] = (
            (1 - self.df_predictions['PredictedMemory']) * 100
        )
        
        # ì˜ˆìƒ ì ˆê°ì•¡ (ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ ë¦¬ì†ŒìŠ¤ì˜ 60% ì ˆê° ê°€ì •)
        cost_col = 'TotalHourlyCost' if 'TotalHourlyCost' in self.df_predictions.columns else 'BilledCost'
        if cost_col in self.df_predictions.columns:
            self.df_predictions['PotentialSavings'] = np.where(
                self.df_predictions['IsOverProvisioned'],
                pd.to_numeric(self.df_predictions[cost_col], errors='coerce').fillna(0) * 0.6,
                0
            )
        
        # ê²°ê³¼ í†µê³„
        self._print_prediction_summary()
        
        return self
    
    
    def _print_prediction_summary(self):
        """
        ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        """
        print(f"\n{'='*100}")
        print("ğŸ“Š AWS ì‚¬ìš©ë¥  ì˜ˆì¸¡ ê²°ê³¼ (XGBoost)")
        print(f"{'='*100}")
        
        total = len(self.df_predictions)
        over_prov = self.df_predictions['IsOverProvisioned'].sum()
        over_prov_rate = over_prov / total * 100
        
        print(f"\n   ğŸš¨ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ íƒì§€:")
        print(f"      â€¢ ì „ì²´: {total:,}ê±´")
        print(f"      â€¢ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {over_prov:,}ê±´ ({over_prov_rate:.1f}%)")
        print(f"      â€¢ ì •ìƒ: {total - over_prov:,}ê±´")
        
        # ì˜ˆì¸¡ê°’ ë¶„í¬
        print(f"\n   ğŸ“Š ì˜ˆì¸¡ ì‚¬ìš©ë¥  ë¶„í¬:")
        print(f"      â€¢ CPU í‰ê· : {self.df_predictions['PredictedCPU'].mean()*100:.1f}%")
        print(f"      â€¢ CPU ì¤‘ì•™ê°’: {self.df_predictions['PredictedCPU'].median()*100:.1f}%")
        print(f"      â€¢ Memory í‰ê· : {self.df_predictions['PredictedMemory'].mean()*100:.1f}%")
        print(f"      â€¢ Memory ì¤‘ì•™ê°’: {self.df_predictions['PredictedMemory'].median()*100:.1f}%")
        
        # ì˜ˆìƒ ì ˆê°ì•¡
        if 'PotentialSavings' in self.df_predictions.columns:
            total_savings = self.df_predictions['PotentialSavings'].sum()
            print(f"\n   ğŸ’° ì˜ˆìƒ ì ˆê°ì•¡:")
            print(f"      â€¢ ì´ ì ˆê° ê°€ëŠ¥: ${total_savings:,.2f}")
            print(f"      â€¢ ì—°ê°„ ì¶”ì •: ${total_savings * 12:,.2f}")
        
        print(f"\n{'='*100}")
    
    
    def save(self):
        """
        ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        
        Returns:
            self: ë©”ì„œë“œ ì²´ì´ë‹ìš©
        """
        if self.df_predictions is None:
            self.print_warning("ì €ì¥í•  ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return self
        
        self.print_step("ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥", f"{self.result_output_path}")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.ensure_dir(self.result_output_path.parent)
        
        # CSV ì €ì¥
        self.df_predictions.to_csv(self.result_output_path, index=False)
        
        self.print_success("ì €ì¥ ì™„ë£Œ")
        print(f"   ğŸ“‚ ê²½ë¡œ: {self.result_output_path}")
        print(f"   ğŸ“Š ë ˆì½”ë“œ: {len(self.df_predictions):,}ê±´")
        
        # ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ë§Œ ë³„ë„ ì €ì¥
        over_prov_path = self.result_output_path.parent / 'xgb_overprovisioned.csv'
        df_over = self.df_predictions[self.df_predictions['IsOverProvisioned']]
        
        if len(df_over) > 0:
            df_over.to_csv(over_prov_path, index=False)
            print(f"   ğŸ“‚ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {over_prov_path}")
            print(f"   ğŸ“Š ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {len(df_over):,}ê±´")
        
        return self
    
    
    def run(self):
        """
        ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰: ë¡œë“œ â†’ í•™ìŠµ â†’ ì˜ˆì¸¡ â†’ ì €ì¥
        
        Returns:
            self: ë©”ì„œë“œ ì²´ì´ë‹ìš©
        """
        return (self.load()
                .process()
                .predict_aws()
                .save())
    
    
    def get_results(self):
        """
        ê²°ê³¼ ë°˜í™˜
        
        Returns:
            tuple: (ì˜ˆì¸¡ ê²°ê³¼ DataFrame, í•™ìŠµ ê²°ê³¼ dict)
        """
        return (self.df_predictions, self.training_results)
    
    
    def get_overprovisioned(self):
        """
        ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ ë°ì´í„°ë§Œ ë°˜í™˜
        
        Returns:
            DataFrame: ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ ë°ì´í„°
        """
        if self.df_predictions is None:
            return None
        
        return self.df_predictions[self.df_predictions['IsOverProvisioned']].copy()
    
    
    def compare_with_rf(self, rf_results_path=None):
        """
        RandomForest ê²°ê³¼ì™€ ë¹„êµ
        
        Args:
            rf_results_path: RF ê²°ê³¼ JSON ê²½ë¡œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        
        Returns:
            dict: ë¹„êµ ê²°ê³¼
        """
        if self.training_results is None:
            print("âš ï¸ ë¨¼ì € í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        # RandomForest ê¸°ì¡´ ê²°ê³¼ (ì´ì „ ëŒ€í™”ì—ì„œ í™•ì¸ëœ ê°’)
        rf_results = {
            'cpu_mae': 0.2369,
            'cpu_r2': 0.0895,
            'memory_mae': 0.2371,
            'memory_r2': 0.0947
        }
        
        comparison = {
            'model_comparison': 'XGBoost vs RandomForest',
            'cpu': {
                'xgb_mae': self.training_results['cpu_mae'],
                'rf_mae': rf_results['cpu_mae'],
                'mae_improvement': rf_results['cpu_mae'] - self.training_results['cpu_mae'],
                'xgb_r2': self.training_results['cpu_r2'],
                'rf_r2': rf_results['cpu_r2'],
                'r2_improvement': self.training_results['cpu_r2'] - rf_results['cpu_r2']
            },
            'memory': {
                'xgb_mae': self.training_results['memory_mae'],
                'rf_mae': rf_results['memory_mae'],
                'mae_improvement': rf_results['memory_mae'] - self.training_results['memory_mae'],
                'xgb_r2': self.training_results['memory_r2'],
                'rf_r2': rf_results['memory_r2'],
                'r2_improvement': self.training_results['memory_r2'] - rf_results['memory_r2']
            }
        }
        
        return comparison


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    
    print("\nğŸš€ XGBoost ê¸°ë°˜ ì‚¬ìš©ë¥  ì˜ˆì¸¡ ì‹œì‘")
    print("="*100)
    print("ğŸ“Œ ë…¼ë¬¸: LLM ê¸°ë°˜ í´ë¼ìš°ë“œ FinOps ìë™í™” ì‹œìŠ¤í…œ - Gemini vs Claude ì„±ëŠ¥ ë¹„êµ")
    print("ğŸ“Œ ëª©ì : RandomForest ëŒ€ë¹„ XGBoost ì„±ëŠ¥ ë¹„êµ")
    print("="*100)
    
    # ì˜ˆì¸¡ê¸° ìƒì„±
    predictor = XGBUsagePredictor('config/focus_config.yaml')
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì„¤ì •
    predictor.tune_hyperparams = True
    predictor.sample_size = 5_000_000
    predictor.n_iter = 15
    predictor.cv_folds = 3
    
    # ì‹¤í–‰
    predictor.run()
    
    # ê²°ê³¼ ì¡°íšŒ
    df_predictions, training_results = predictor.get_results()
    
    print(f"\nâœ… ì™„ë£Œ!")
    if df_predictions is not None:
        print(f"   ì „ì²´ ì˜ˆì¸¡: {len(df_predictions):,}ê±´")
        
        df_over = predictor.get_overprovisioned()
        if df_over is not None:
            print(f"   ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹: {len(df_over):,}ê±´")
    
    # RF ëŒ€ë¹„ ë¹„êµ
    comparison = predictor.compare_with_rf()
    if comparison:
        print(f"\nğŸ“ˆ RandomForest ëŒ€ë¹„ ë¹„êµ:")
        print(f"   CPU MAE ê°œì„ : {comparison['cpu']['mae_improvement']*100:.2f}%p")
        print(f"   CPU RÂ² ê°œì„ : {comparison['cpu']['r2_improvement']:.4f}")
        print(f"   Memory MAE ê°œì„ : {comparison['memory']['mae_improvement']*100:.2f}%p")
        print(f"   Memory RÂ² ê°œì„ : {comparison['memory']['r2_improvement']:.4f}")